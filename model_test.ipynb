{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae49697",
   "metadata": {},
   "source": [
    "## This notebook is to test a random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2c8f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3638b609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready. Train path: Quentin/dsba-m-1-challenge-purchase-prediction/train_dataset_M1_with_id.csv\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Basic imports and path config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Filepath you provided (train)\n",
    "TRAIN_PATH = \"Quentin/dsba-m-1-challenge-purchase-prediction/train_dataset_M1_with_id.csv\"\n",
    "# (We will not load test for model selection — only for final prediction later if allowed)\n",
    "# TEST_PATH = \"/Users/quentinvillet/oracles_of_paris/kaggle_competition/Quentin/dsba-m-1-challenge-purchase-prediction/test_dataset_M1_with_id.csv\"\n",
    "\n",
    "RND = 42\n",
    "print(\"Ready. Train path:\", TRAIN_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6abb5b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 13735\n",
      "After filtering to days ≤70 rows: 13735\n",
      "(13735, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Reviews_Read</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Category</th>\n",
       "      <th>Items_In_Cart</th>\n",
       "      <th>Time_of_Day</th>\n",
       "      <th>Email_Interaction</th>\n",
       "      <th>Device_Type</th>\n",
       "      <th>Payment_Method</th>\n",
       "      <th>Referral_Source</th>\n",
       "      <th>Socioeconomic_Status_Score</th>\n",
       "      <th>Engagement_Score</th>\n",
       "      <th>AB_Bucket</th>\n",
       "      <th>Price_Sine</th>\n",
       "      <th>PM_RS_Combo</th>\n",
       "      <th>Day</th>\n",
       "      <th>Campaign_Period</th>\n",
       "      <th>Purchase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>592.975</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>afterno0n</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Credit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.26</td>\n",
       "      <td>1.856520</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.999047</td>\n",
       "      <td>Credit:Social_media</td>\n",
       "      <td>59</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>511.279</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>morning</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Social_media</td>\n",
       "      <td>8.30</td>\n",
       "      <td>1.868138</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.129689</td>\n",
       "      <td>Cash:Social_media</td>\n",
       "      <td>29</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>218.360</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>evening</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Bank</td>\n",
       "      <td>Social_media</td>\n",
       "      <td>6.61</td>\n",
       "      <td>1.223445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.421646</td>\n",
       "      <td>Bank:Social_media</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>313.781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>evening</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>pay pal</td>\n",
       "      <td>Social_media</td>\n",
       "      <td>10.51</td>\n",
       "      <td>0.359684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.988239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>495.088</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>evening</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Social_media</td>\n",
       "      <td>8.33</td>\n",
       "      <td>3.848580</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.695737</td>\n",
       "      <td>Cash:Social_media</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender  Reviews_Read    Price  Discount  Category  Items_In_Cart  \\\n",
       "0   NaN     1.0           3.0  592.975      22.0       1.0            6.0   \n",
       "1  25.0     1.0           1.0  511.279      12.0       0.0            3.0   \n",
       "2  22.0     0.0           3.0  218.360       2.0       1.0            4.0   \n",
       "3  24.0     0.0           3.0  313.781       1.0       3.0            0.0   \n",
       "4  35.0     1.0           1.0  495.088      13.0       0.0            2.0   \n",
       "\n",
       "  Time_of_Day  Email_Interaction Device_Type Payment_Method Referral_Source  \\\n",
       "0   afterno0n                0.0      Mobile         Credit             NaN   \n",
       "1     morning                1.0      Tablet           Cash    Social_media   \n",
       "2     evening                1.0      Mobile           Bank    Social_media   \n",
       "3     evening                1.0      Mobile        pay pal    Social_media   \n",
       "4     evening                0.0      Mobile           Cash    Social_media   \n",
       "\n",
       "   Socioeconomic_Status_Score  Engagement_Score  AB_Bucket  Price_Sine  \\\n",
       "0                        7.26          1.856520        3.0    0.999047   \n",
       "1                        8.30          1.868138        5.0   -0.129689   \n",
       "2                        6.61          1.223445        0.0   -0.421646   \n",
       "3                       10.51          0.359684        1.0   -0.988239   \n",
       "4                        8.33          3.848580        2.0    0.695737   \n",
       "\n",
       "           PM_RS_Combo  Day Campaign_Period  Purchase  \n",
       "0  Credit:Social_media   59           False         0  \n",
       "1    Cash:Social_media   29            True         1  \n",
       "2    Bank:Social_media   16           False         0  \n",
       "3                  NaN   53           False         0  \n",
       "4    Cash:Social_media   10           False         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "print(\"Original rows:\", len(df))\n",
    "# enforce only days <= 70 (competition rule)\n",
    "df = df[df['Day'] <= 70].reset_index(drop=True)\n",
    "print(\"After filtering to days ≤70 rows:\", len(df))\n",
    "\n",
    "# drop columns you already removed (repeat safe drop if needed)\n",
    "drop_cols = ['id', 'Session_ID', 'PM_from_combo', 'RS_from_combo', 'Items_In_Cart_raw']\n",
    "for c in drop_cols:\n",
    "    if c in df.columns:\n",
    "        df.drop(columns=c, inplace=True)\n",
    "\n",
    "# quick look\n",
    "print(df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f14123e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Time_of_Day values: ['afternoon' 'morning' 'evening' 'nan' 'mrning']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def clean_strings(df):\n",
    "    df = df.copy()\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "        df[col] = df[col].replace({'nan': np.nan, 'none': np.nan, '': np.nan})\n",
    "    # Time_of_Day variants -> morning / afternoon / evening\n",
    "    if 'Time_of_Day' in df.columns:\n",
    "        tod = df['Time_of_Day'].astype(str).str.lower()\n",
    "        tod = tod.str.replace(r'[^a-z]', '', regex=True)\n",
    "        tod = tod.replace({\n",
    "            r'^morn.*': 'morning',\n",
    "            r'^aft.*': 'afternoon',\n",
    "            r'^even.*': 'evening',\n",
    "            r'^nig.*': 'evening',\n",
    "            r'^mid.*': 'afternoon'\n",
    "        }, regex=True)\n",
    "        df['Time_of_Day'] = tod\n",
    "    # Device_Type heuristics\n",
    "    if 'Device_Type' in df.columns:\n",
    "        dev = df['Device_Type'].astype(str).str.replace(r'[^a-z]', '', regex=True)\n",
    "        dev = dev.replace({\n",
    "            r'^phone.*': 'mobile',\n",
    "            r'^mob.*': 'mobile',\n",
    "            r'^lap.*': 'desktop',\n",
    "            r'^desk.*': 'desktop',\n",
    "            r'^tab.*': 'tablet'\n",
    "        }, regex=True)\n",
    "        df['Device_Type'] = dev\n",
    "    # Payment method normalization\n",
    "    if 'Payment_Method' in df.columns:\n",
    "        pm = df['Payment_Method'].astype(str)\n",
    "        pm = pm.replace({\n",
    "            r'^cred.*': 'credit',\n",
    "            r'^debit.*': 'debit',\n",
    "            r'^pay[\\s_]?pal$': 'paypal',\n",
    "            r'^cash$': 'cash',\n",
    "            r'^bank.*': 'bank'\n",
    "        }, regex=True)\n",
    "        df['Payment_Method'] = pm\n",
    "    # Referral source\n",
    "    if 'Referral_Source' in df.columns:\n",
    "        rs = df['Referral_Source'].astype(str)\n",
    "        rs = rs.replace({\n",
    "            r'social.*': 'social_media',\n",
    "            r'search.*': 'search_engine',\n",
    "            r'^ads?$': 'ads',\n",
    "            r'^email$': 'email',\n",
    "            r'^direct$': 'direct'\n",
    "        }, regex=True)\n",
    "        df['Referral_Source'] = rs\n",
    "    return df\n",
    "\n",
    "df = clean_strings(df)\n",
    "print(\"Unique Time_of_Day values:\", df['Time_of_Day'].dropna().unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f23d60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Effective_Price</th>\n",
       "      <th>Has_Discount</th>\n",
       "      <th>High_Discount</th>\n",
       "      <th>Price_Category</th>\n",
       "      <th>Items_In_Cart</th>\n",
       "      <th>Items_In_Cart_bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>592.975</td>\n",
       "      <td>22.0</td>\n",
       "      <td>462.52050</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Premium</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>511.279</td>\n",
       "      <td>12.0</td>\n",
       "      <td>449.92552</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Premium</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>218.360</td>\n",
       "      <td>2.0</td>\n",
       "      <td>213.99280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>High</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>313.781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.64319</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>High</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>495.088</td>\n",
       "      <td>13.0</td>\n",
       "      <td>430.72656</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>High</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Discount  Effective_Price  Has_Discount  High_Discount  \\\n",
       "0  592.975      22.0        462.52050             1              0   \n",
       "1  511.279      12.0        449.92552             1              0   \n",
       "2  218.360       2.0        213.99280             1              0   \n",
       "3  313.781       1.0        310.64319             1              0   \n",
       "4  495.088      13.0        430.72656             1              0   \n",
       "\n",
       "  Price_Category  Items_In_Cart Items_In_Cart_bucket  \n",
       "0        Premium            6.0                 5-10  \n",
       "1        Premium            3.0                  3-4  \n",
       "2           High            4.0                  3-4  \n",
       "3           High            0.0                    0  \n",
       "4           High            2.0                    2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "def fe_price_items(df):\n",
    "    df = df.copy()\n",
    "    # ensure numeric\n",
    "    df['Price'] = pd.to_numeric(df['Price'], errors='coerce').fillna(0)\n",
    "    df['Discount'] = pd.to_numeric(df['Discount'], errors='coerce').fillna(0)\n",
    "    # derived\n",
    "    df['Effective_Price'] = df['Price'] * (1 - df['Discount']/100.0)\n",
    "    df['Discount_Amount'] = df['Price'] * df['Discount'] / 100.0\n",
    "    df['Has_Discount'] = (df['Discount'] > 0).astype(int)\n",
    "    df['High_Discount'] = (df['Discount'] > 30).astype(int)\n",
    "    # price category bins (tweak thresholds if needed)\n",
    "    df['Price_Category'] = pd.cut(df['Price'], bins=[-1, 50, 200, 500, 100000], labels=['Low','Medium','High','Premium'])\n",
    "    # Items in cart: safe buckets\n",
    "    df['Items_In_Cart'] = pd.to_numeric(df['Items_In_Cart'], errors='coerce').fillna(0)\n",
    "    df['Has_Items'] = (df['Items_In_Cart'] > 0).astype(int)\n",
    "    df['Items_In_Cart_bucket'] = pd.cut(df['Items_In_Cart'], bins=[-1,0,1,2,4,10,np.inf], labels=['0','1','2','3-4','5-10','10+'])\n",
    "    return df\n",
    "\n",
    "df = fe_price_items(df)\n",
    "display(df[['Price','Discount','Effective_Price','Has_Discount','High_Discount','Price_Category','Items_In_Cart','Items_In_Cart_bucket']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bf128ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed PM_RS_Combo where applicable.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if 'PM_RS_Combo' in df.columns:\n",
    "    mask = df['PM_RS_Combo'].notna() & ((df['Payment_Method'].isna()) | (df['Referral_Source'].isna()))\n",
    "    if mask.any():\n",
    "        split = df.loc[mask, 'PM_RS_Combo'].str.split(':', expand=True)\n",
    "        df.loc[mask, 'Payment_Method'] = df.loc[mask, 'Payment_Method'].fillna(split[0])\n",
    "        df.loc[mask, 'Referral_Source'] = df.loc[mask, 'Referral_Source'].fillna(split[1])\n",
    "    # drop original combo (avoid duplication)\n",
    "    df.drop(columns=['PM_RS_Combo'], inplace=True, errors='ignore')\n",
    "    print(\"Parsed PM_RS_Combo where applicable.\")\n",
    "else:\n",
    "    print(\"No PM_RS_Combo column found; skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8f54ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping these columns (to reduce noise): ['Payment_Method', 'AB_Bucket', 'Price_Sine', 'Campaign_Period', 'Effective_Price', 'Discount_Amount']\n",
      "Remaining columns: ['Age', 'Gender', 'Reviews_Read', 'Price', 'Discount', 'Category', 'Items_In_Cart', 'Time_of_Day', 'Email_Interaction', 'Device_Type', 'Referral_Source', 'Socioeconomic_Status_Score', 'Engagement_Score', 'Day', 'Purchase', 'Has_Discount', 'High_Discount', 'Price_Category', 'Has_Items', 'Items_In_Cart_bucket']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Final recommended feature list (safe)\n",
    "final_features = [\n",
    "    'Age', 'Gender', 'Category', 'Price', 'Discount', 'Reviews_Read',\n",
    "    'Items_In_Cart', 'Items_In_Cart_bucket', 'Has_Items',\n",
    "    'Time_of_Day', 'Email_Interaction', 'Device_Type', 'Referral_Source',\n",
    "    'Socioeconomic_Status_Score', 'Engagement_Score',\n",
    "    'Has_Discount', 'High_Discount', 'Price_Category'\n",
    "]\n",
    "\n",
    "# Drop any features not in df (safe)\n",
    "final_features = [f for f in final_features if f in df.columns]\n",
    "\n",
    "# Optionally drop features we don't want\n",
    "to_drop = [c for c in df.columns if c not in final_features + ['Purchase','Day']]\n",
    "print(\"Dropping these columns (to reduce noise):\", to_drop[:50])\n",
    "df = df.drop(columns=to_drop)\n",
    "print(\"Remaining columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "573068f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts by Items_In_Cart (first 20):\n",
      "0.0     1501\n",
      "1.0     2312\n",
      "2.0     2410\n",
      "3.0     2046\n",
      "4.0     1546\n",
      "5.0     1243\n",
      "6.0      837\n",
      "7.0      578\n",
      "8.0      430\n",
      "9.0      273\n",
      "10.0     191\n",
      "11.0     120\n",
      "12.0      83\n",
      "13.0      60\n",
      "14.0      33\n",
      "15.0      33\n",
      "16.0      12\n",
      "17.0      10\n",
      "18.0       7\n",
      "19.0       4\n",
      "Name: Items_In_Cart, dtype: int64\n",
      "\n",
      "Purchase rate by Items_In_Cart (first 20):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Items_In_Cart</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.769487</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.695934</td>\n",
       "      <td>2312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.508299</td>\n",
       "      <td>2410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.347507</td>\n",
       "      <td>2046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.161061</td>\n",
       "      <td>1546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.065969</td>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0.022700</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>0.006920</td>\n",
       "      <td>578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>0.002326</td>\n",
       "      <td>430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>0.003663</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mean  count\n",
       "Items_In_Cart                 \n",
       "0.0            0.769487   1501\n",
       "1.0            0.695934   2312\n",
       "2.0            0.508299   2410\n",
       "3.0            0.347507   2046\n",
       "4.0            0.161061   1546\n",
       "5.0            0.065969   1243\n",
       "6.0            0.022700    837\n",
       "7.0            0.006920    578\n",
       "8.0            0.002326    430\n",
       "9.0            0.003663    273\n",
       "10.0           0.000000    191\n",
       "11.0           0.000000    120\n",
       "12.0           0.000000     83\n",
       "13.0           0.000000     60\n",
       "14.0           0.000000     33\n",
       "15.0           0.000000     33\n",
       "16.0           0.000000     12\n",
       "17.0           0.000000     10\n",
       "18.0           0.000000      7\n",
       "19.0           0.000000      4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "print(\"Counts by Items_In_Cart (first 20):\")\n",
    "print(df['Items_In_Cart'].value_counts().sort_index().head(20))\n",
    "\n",
    "print(\"\\nPurchase rate by Items_In_Cart (first 20):\")\n",
    "display(df.groupby('Items_In_Cart')['Purchase'].agg(['mean','count']).sort_index().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "285e862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Items_In_Cart', 'Discount', 'High_Discount', 'Price_Category']\n",
    "df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dfa13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = [\n",
    "    'Age',\n",
    "    'Gender',\n",
    "    'Reviews_Read',\n",
    "    'Price',\n",
    "    'Category',\n",
    "    'Time_of_Day',\n",
    "    'Email_Interaction',\n",
    "    'Device_Type',\n",
    "    'Referral_Source',\n",
    "    'Socioeconomic_Status_Score',\n",
    "    'Engagement_Score',\n",
    "    'Has_Discount',\n",
    "    'Has_Items',\n",
    "    'Items_In_Cart_bucket'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50257d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['Time_of_Day', 'Device_Type', 'Referral_Source', 'Items_In_Cart_bucket']\n",
      "Numeric columns: ['Age', 'Gender', 'Reviews_Read', 'Price', 'Category', 'Email_Interaction', 'Socioeconomic_Status_Score', 'Engagement_Score', 'Has_Discount', 'Has_Items']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# FINAL FEATURE LIST\n",
    "final_features = [\n",
    "    'Age',\n",
    "    'Gender',\n",
    "    'Reviews_Read',\n",
    "    'Price',\n",
    "    'Category',\n",
    "    'Time_of_Day',\n",
    "    'Email_Interaction',\n",
    "    'Device_Type',\n",
    "    'Referral_Source',\n",
    "    'Socioeconomic_Status_Score',\n",
    "    'Engagement_Score',\n",
    "    'Has_Discount',\n",
    "    'Has_Items',\n",
    "    'Items_In_Cart_bucket'\n",
    "]\n",
    "\n",
    "# CATEGORICAL / NUMERIC SPLIT\n",
    "cat_cols = [c for c in final_features if df[c].dtype == 'object' or str(df[c].dtype).startswith('category')]\n",
    "num_cols = [c for c in final_features if c not in cat_cols]\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "\n",
    "# ---- PREPROCESSOR CLASS ----\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, cat_cols, num_cols, n_neighbors=5):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.n_neighbors = n_neighbors\n",
    "\n",
    "        self.cat_imputer = None\n",
    "        self.ohe = None\n",
    "        self.knn = None\n",
    "        self.scaler = None\n",
    "        self.ohe_cols = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Impute categoricals\n",
    "        self.cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        X_cat = self.cat_imputer.fit_transform(X[self.cat_cols])\n",
    "\n",
    "        # One-hot encode\n",
    "        self.ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        arr = self.ohe.fit_transform(X_cat)\n",
    "        self.ohe_cols = list(self.ohe.get_feature_names_out(self.cat_cols))\n",
    "\n",
    "        # Prepare for KNN-imputation\n",
    "        X_num = X[self.num_cols].copy()\n",
    "        X_knn_fit = pd.concat(\n",
    "            [pd.DataFrame(arr, index=X.index, columns=self.ohe_cols),\n",
    "             X_num],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Fit KNN-imputer\n",
    "        self.knn = KNNImputer(n_neighbors=self.n_neighbors, weights='distance')\n",
    "        self.knn.fit(X_knn_fit)\n",
    "\n",
    "        # Fit scaler\n",
    "        X_knn_imp = pd.DataFrame(\n",
    "            self.knn.transform(X_knn_fit),\n",
    "            columns=X_knn_fit.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.scaler.fit(X_knn_imp)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Process categoricals\n",
    "        X_cat = self.cat_imputer.transform(X[self.cat_cols])\n",
    "        arr = self.ohe.transform(X_cat)\n",
    "        df_ohe = pd.DataFrame(arr, index=X.index, columns=self.ohe_cols)\n",
    "\n",
    "        # Process numericals\n",
    "        X_num = X[self.num_cols].copy()\n",
    "\n",
    "        # Combine for final imputing\n",
    "        X_knn = pd.concat([df_ohe, X_num], axis=1)\n",
    "\n",
    "        # Order columns\n",
    "        X_knn = X_knn[self.ohe_cols + self.num_cols]\n",
    "\n",
    "        # Apply KNN imputation\n",
    "        X_imp = pd.DataFrame(\n",
    "            self.knn.transform(X_knn),\n",
    "            columns=X_knn.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "        # Scale\n",
    "        X_scaled = pd.DataFrame(\n",
    "            self.scaler.transform(X_imp),\n",
    "            columns=X_imp.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "\n",
    "        return X_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f82c627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['Day'] <= 70].copy()\n",
    "valid_df = df[df['Day'] > 70].copy()\n",
    "\n",
    "X_train = train_df[final_features].copy()\n",
    "y_train = train_df['Purchase'].copy()\n",
    "\n",
    "X_valid = valid_df[final_features].copy()\n",
    "y_valid = valid_df['Purchase'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54495ee8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required by SimpleImputer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m PP\u001b[38;5;241m.\u001b[39mfit(X_train)\n\u001b[1;32m      4\u001b[0m X_train_prep \u001b[38;5;241m=\u001b[39m PP\u001b[38;5;241m.\u001b[39mtransform(X_train)\n\u001b[0;32m----> 5\u001b[0m X_valid_prep \u001b[38;5;241m=\u001b[39m \u001b[43mPP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 85\u001b[0m, in \u001b[0;36mPreprocessor.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     82\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Process categoricals\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m X_cat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_imputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mohe\u001b[38;5;241m.\u001b[39mtransform(X_cat)\n\u001b[1;32m     87\u001b[0m df_ohe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(arr, index\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mohe_cols)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/HarmonIQ/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/HarmonIQ/lib/python3.10/site-packages/sklearn/impute/_base.py:625\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    `X` with imputed values.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 625\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatistics_\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/HarmonIQ/lib/python3.10/site-packages/sklearn/impute/_base.py:379\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_fit:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_dtype \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/HarmonIQ/lib/python3.10/site-packages/sklearn/impute/_base.py:360\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    357\u001b[0m     ensure_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not convert\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/HarmonIQ/lib/python3.10/site-packages/sklearn/utils/validation.py:2954\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2952\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2954\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2956\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/HarmonIQ/lib/python3.10/site-packages/sklearn/utils/validation.py:1128\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1128\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1130\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1131\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1132\u001b[0m         )\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1135\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 4)) while a minimum of 1 is required by SimpleImputer."
     ]
    }
   ],
   "source": [
    "PP = Preprocessor(cat_cols, num_cols)\n",
    "PP.fit(X_train)\n",
    "\n",
    "X_train_prep = PP.transform(X_train)\n",
    "X_valid_prep = PP.transform(X_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b1af49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df['Day'].max())\n",
    "print(df['Day'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5d7e597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature candidates: ['Age', 'Gender', 'Reviews_Read', 'Price', 'Discount', 'Category', 'Time_of_Day', 'Email_Interaction', 'Device_Type', 'Payment_Method', 'Referral_Source', 'Socioeconomic_Status_Score', 'Engagement_Score', 'AB_Bucket', 'Price_Sine', 'PM_RS_Combo', 'Campaign_Period']\n",
      "Categorical cols: ['Time_of_Day', 'Device_Type', 'Payment_Method', 'Referral_Source', 'PM_RS_Combo', 'Campaign_Period']\n",
      "Numeric cols: ['Age', 'Gender', 'Reviews_Read', 'Price', 'Discount', 'Category', 'Email_Interaction', 'Socioeconomic_Status_Score', 'Engagement_Score', 'AB_Bucket', 'Price_Sine']\n",
      "Preprocessing done. Shape: (13735, 173)\n",
      "Random Forest 5-fold CV Accuracy: 0.6823 ± 0.0048\n",
      "Random Forest 5-fold CV F1-score: 0.3648 ± 0.0136\n",
      "Random Forest 5-fold CV ROC-AUC: 0.7230 ± 0.0103\n",
      "Random Forest trained on full dataset.\n",
      "                           feature  importance\n",
      "165                          Price    0.138072\n",
      "170               Engagement_Score    0.123536\n",
      "172                     Price_Sine    0.075595\n",
      "169     Socioeconomic_Status_Score    0.074264\n",
      "164                   Reviews_Read    0.069684\n",
      "166                       Discount    0.065840\n",
      "162                            Age    0.063337\n",
      "167                       Category    0.050034\n",
      "171                      AB_Bucket    0.041160\n",
      "168              Email_Interaction    0.035472\n",
      "161           Campaign_Period_True    0.017539\n",
      "58              Device_Type_Mobile    0.016794\n",
      "163                         Gender    0.016463\n",
      "160          Campaign_Period_False    0.016094\n",
      "48             Time_of_Day_evening    0.012305\n",
      "36           Time_of_Day_afternoon    0.011220\n",
      "56             Time_of_Day_morning    0.010952\n",
      "57             Device_Type_Desktop    0.010834\n",
      "63             Payment_Method_Bank    0.010408\n",
      "126  Referral_Source_Search_engine    0.010217\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load the train dataset\n",
    "df = pd.read_csv('Quentin/dsba-m-1-challenge-purchase-prediction/train_dataset_M1_with_id.csv')\n",
    "\n",
    "# %%\n",
    "# Columns to drop\n",
    "drop_cols = ['id', 'Session_ID', 'PM_from_combo', 'RS_from_combo', 'Items_In_Cart']  # dropped Items_In_Cart\n",
    "for c in drop_cols:\n",
    "    if c in df.columns:\n",
    "        df.drop(columns=c, inplace=True)\n",
    "\n",
    "# Final features to use (everything except target and day)\n",
    "target = 'Purchase'\n",
    "final_features = [c for c in df.columns if c != target and c != 'Day']\n",
    "print(\"Feature candidates:\", final_features)\n",
    "\n",
    "# %%\n",
    "# Define categorical and numeric columns\n",
    "cat_cols = [c for c in final_features if df[c].dtype == 'object' or str(df[c].dtype).startswith('category')]\n",
    "num_cols = [c for c in final_features if c not in cat_cols]\n",
    "\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "\n",
    "# %%\n",
    "# Preprocessor class\n",
    "class Preprocessor:\n",
    "    def __init__(self, cat_cols, num_cols, n_neighbors=5):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.num_cols = num_cols\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.cat_imputer = None\n",
    "        self.ohe = None\n",
    "        self.knn = None\n",
    "        self.scaler = None\n",
    "        self.ohe_cols = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Categorical imputation\n",
    "        self.cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        X_cat = X[self.cat_cols].copy()\n",
    "        X_cat = self.cat_imputer.fit_transform(X_cat)\n",
    "\n",
    "        # One-hot encoding\n",
    "        self.ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        arr = self.ohe.fit_transform(X_cat)\n",
    "        self.ohe_cols = list(self.ohe.get_feature_names_out(self.cat_cols))\n",
    "\n",
    "        # Numeric imputation using KNN (with OHE included)\n",
    "        X_num = X[self.num_cols].copy()\n",
    "        X_knn_fit = pd.concat([pd.DataFrame(arr, index=X.index, columns=self.ohe_cols), X_num], axis=1)\n",
    "        self.knn = KNNImputer(n_neighbors=self.n_neighbors, weights='distance')\n",
    "        self.knn.fit(X_knn_fit)\n",
    "\n",
    "        # Scaler\n",
    "        X_knn_imp = pd.DataFrame(self.knn.transform(X_knn_fit), columns=X_knn_fit.columns, index=X.index)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.scaler.fit(X_knn_imp)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X_cat = X[self.cat_cols].copy()\n",
    "        X_cat = self.cat_imputer.transform(X_cat)\n",
    "        arr = self.ohe.transform(X_cat)\n",
    "        df_ohe = pd.DataFrame(arr, index=X.index, columns=self.ohe_cols)\n",
    "\n",
    "        X_num = X[self.num_cols].copy()\n",
    "        X_knn = pd.concat([df_ohe, X_num], axis=1)\n",
    "\n",
    "        # Ensure all OHE columns exist\n",
    "        for c in [c for c in self.ohe_cols if c not in X_knn.columns]:\n",
    "            X_knn[c] = 0.0\n",
    "\n",
    "        X_knn = X_knn[self.ohe_cols + self.num_cols]\n",
    "        X_imp = pd.DataFrame(self.knn.transform(X_knn), columns=X_knn.columns, index=X.index)\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X_imp), columns=X_imp.columns, index=X.index)\n",
    "        return X_scaled\n",
    "\n",
    "# %%\n",
    "# Separate features and target\n",
    "X = df[final_features]\n",
    "y = df[target]\n",
    "\n",
    "# Fit preprocessor\n",
    "PP = Preprocessor(cat_cols, num_cols)\n",
    "PP.fit(X)\n",
    "\n",
    "# Transform features\n",
    "X_prep = PP.transform(X)\n",
    "\n",
    "print(\"Preprocessing done. Shape:\", X_prep.shape)\n",
    "\n",
    "# %%\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "acc_scores = cross_val_score(rf, X_prep, y, cv=cv, scoring='accuracy')\n",
    "f1_scores = cross_val_score(rf, X_prep, y, cv=cv, scoring='f1')\n",
    "roc_scores = cross_val_score(rf, X_prep, y, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"Random Forest 5-fold CV Accuracy: {acc_scores.mean():.4f} ± {acc_scores.std():.4f}\")\n",
    "print(f\"Random Forest 5-fold CV F1-score: {f1_scores.mean():.4f} ± {f1_scores.std():.4f}\")\n",
    "print(f\"Random Forest 5-fold CV ROC-AUC: {roc_scores.mean():.4f} ± {roc_scores.std():.4f}\")\n",
    "\n",
    "# %%\n",
    "# Fit final model on all data\n",
    "rf.fit(X_prep, y)\n",
    "print(\"Random Forest trained on full dataset.\")\n",
    "\n",
    "# %%\n",
    "# Feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X_prep.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(importances.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49b415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HarmonIQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
