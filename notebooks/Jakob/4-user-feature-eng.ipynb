{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2b899d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING FRESH: UNDERSTANDING OUR DATA\n",
      "======================================================================\n",
      "\n",
      "Dataset shape: (13735, 53)\n",
      "\n",
      "Columns we have:\n",
      "['id', 'Age', 'Gender', 'Reviews_Read', 'Price', 'Discount', 'Category', 'Items_In_Cart', 'Email_Interaction', 'Socioeconomic_Status_Score', 'Engagement_Score', 'AB_Bucket', 'Price_Sine', 'Day', 'Purchase', 'Time_of_Day_afternoon', 'Time_of_Day_evening', 'Time_of_Day_morning', 'Device_Type_desktop', 'Device_Type_mobile', 'Device_Type_tablet', 'Payment_Method_bank', 'Payment_Method_cash', 'Payment_Method_credit', 'Payment_Method_paypal', 'Referral_Source_ads', 'Referral_Source_direct', 'Referral_Source_email', 'Referral_Source_search_engine', 'Referral_Source_social_media', 'PM_RS_Combo_bank:ads', 'PM_RS_Combo_bank:direct', 'PM_RS_Combo_bank:email', 'PM_RS_Combo_bank:search_engine', 'PM_RS_Combo_bank:social_media', 'PM_RS_Combo_cash:ads', 'PM_RS_Combo_cash:direct', 'PM_RS_Combo_cash:email', 'PM_RS_Combo_cash:search_engine', 'PM_RS_Combo_cash:social_media', 'PM_RS_Combo_credit:ads', 'PM_RS_Combo_credit:direct', 'PM_RS_Combo_credit:email', 'PM_RS_Combo_credit:search_engine', 'PM_RS_Combo_credit:social_media', 'PM_RS_Combo_paypal:ads', 'PM_RS_Combo_paypal:direct', 'PM_RS_Combo_paypal:email', 'PM_RS_Combo_paypal:search_engine', 'PM_RS_Combo_paypal:social_media', 'Campaign_Period_false', 'Campaign_Period_true', 'Session_ID']\n",
      "\n",
      "üìä Target Distribution:\n",
      "Purchase\n",
      "0.0    8679\n",
      "1.0    5056\n",
      "Name: count, dtype: int64\n",
      "Purchase\n",
      "0.0    0.631889\n",
      "1.0    0.368111\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "üîç Data Types:\n",
      "float64    51\n",
      "int64       1\n",
      "object      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìà Numeric features (50):\n",
      "  - Age\n",
      "  - Gender\n",
      "  - Reviews_Read\n",
      "  - Price\n",
      "  - Discount\n",
      "  - Category\n",
      "  - Items_In_Cart\n",
      "  - Email_Interaction\n",
      "  - Socioeconomic_Status_Score\n",
      "  - Engagement_Score\n",
      "  - AB_Bucket\n",
      "  - Price_Sine\n",
      "  - Day\n",
      "  - Time_of_Day_afternoon\n",
      "  - Time_of_Day_evening\n",
      "  - Time_of_Day_morning\n",
      "  - Device_Type_desktop\n",
      "  - Device_Type_mobile\n",
      "  - Device_Type_tablet\n",
      "  - Payment_Method_bank\n",
      "  - Payment_Method_cash\n",
      "  - Payment_Method_credit\n",
      "  - Payment_Method_paypal\n",
      "  - Referral_Source_ads\n",
      "  - Referral_Source_direct\n",
      "  - Referral_Source_email\n",
      "  - Referral_Source_search_engine\n",
      "  - Referral_Source_social_media\n",
      "  - PM_RS_Combo_bank:ads\n",
      "  - PM_RS_Combo_bank:direct\n",
      "  - PM_RS_Combo_bank:email\n",
      "  - PM_RS_Combo_bank:search_engine\n",
      "  - PM_RS_Combo_bank:social_media\n",
      "  - PM_RS_Combo_cash:ads\n",
      "  - PM_RS_Combo_cash:direct\n",
      "  - PM_RS_Combo_cash:email\n",
      "  - PM_RS_Combo_cash:search_engine\n",
      "  - PM_RS_Combo_cash:social_media\n",
      "  - PM_RS_Combo_credit:ads\n",
      "  - PM_RS_Combo_credit:direct\n",
      "  - PM_RS_Combo_credit:email\n",
      "  - PM_RS_Combo_credit:search_engine\n",
      "  - PM_RS_Combo_credit:social_media\n",
      "  - PM_RS_Combo_paypal:ads\n",
      "  - PM_RS_Combo_paypal:direct\n",
      "  - PM_RS_Combo_paypal:email\n",
      "  - PM_RS_Combo_paypal:search_engine\n",
      "  - PM_RS_Combo_paypal:social_media\n",
      "  - Campaign_Period_false\n",
      "  - Campaign_Period_true\n",
      "\n",
      "üìã Categorical features (0):\n",
      "\n",
      "‚úÖ Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the cleaned & imputed dataset\n",
    "df = pd.read_csv('/Users/jakobbullinger/Documents/Coding Projects/DSBA/Intro Machine Learning/kaggle_competition/data/cleaned/df_imputed.csv', index_col=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING FRESH: UNDERSTANDING OUR DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns we have:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(f\"\\nüìä Target Distribution:\")\n",
    "print(df['Purchase'].value_counts())\n",
    "print(df['Purchase'].value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\nüîç Data Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Identify feature types\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove non-features\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['Purchase', 'id']]\n",
    "if 'Session_ID' in categorical_cols:\n",
    "    categorical_cols.remove('Session_ID')\n",
    "\n",
    "print(f\"\\nüìà Numeric features ({len(numeric_cols)}):\")\n",
    "for col in numeric_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nüìã Categorical features ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"  - {col}\")\n",
    "    \n",
    "print(f\"\\n‚úÖ Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d029d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QUICK FIX: Convert problematic features to binary\n",
      "======================================================================\n",
      "\n",
      "Email_Interaction unique values: 250\n",
      "Sample values: Email_Interaction\n",
      "0.000000    7352\n",
      "1.000000    6135\n",
      "0.433172       1\n",
      "0.664845       1\n",
      "0.203583       1\n",
      "0.609830       1\n",
      "0.367239       1\n",
      "0.394678       1\n",
      "0.419264       1\n",
      "0.228377       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Email_Engaged distribution:\n",
      "Email_Engaged\n",
      "0    7508\n",
      "1    6227\n",
      "Name: count, dtype: int64\n",
      "Purchase rate by Email_Engaged:\n",
      "Email_Engaged\n",
      "0    0.317528\n",
      "1    0.429099\n",
      "Name: Purchase, dtype: float64\n",
      "\n",
      "Category unique values: 292\n",
      "Category_Clean distribution:\n",
      "Category_Clean\n",
      "1    2845\n",
      "4    2772\n",
      "2    2763\n",
      "0    2690\n",
      "3    2665\n",
      "Name: count, dtype: int64\n",
      "Purchase rate by Category:\n",
      "Category_Clean\n",
      "0    0.426022\n",
      "1    0.400351\n",
      "2    0.410785\n",
      "3    0.308818\n",
      "4    0.293290\n",
      "Name: Purchase, dtype: float64\n",
      "\n",
      "‚úÖ Saved: df_imputed_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('/Users/jakobbullinger/Documents/Coding Projects/DSBA/Intro Machine Learning/kaggle_competition/data/cleaned/df_imputed.csv', index_col=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"QUICK FIX: Convert problematic features to binary\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fix Email_Interaction - make it binary\n",
    "print(\"\\nEmail_Interaction unique values:\", df['Email_Interaction'].nunique())\n",
    "print(\"Sample values:\", df['Email_Interaction'].value_counts().head(10))\n",
    "\n",
    "# Convert to binary (1 if > 0, else 0)\n",
    "df['Email_Engaged'] = (df['Email_Interaction'] > 0.5).astype(int)\n",
    "print(f\"\\nEmail_Engaged distribution:\\n{df['Email_Engaged'].value_counts()}\")\n",
    "print(f\"Purchase rate by Email_Engaged:\\n{df.groupby('Email_Engaged')['Purchase'].mean()}\")\n",
    "\n",
    "# Fix Category - round to nearest integer\n",
    "print(\"\\nCategory unique values:\", df['Category'].nunique())\n",
    "df['Category_Clean'] = df['Category'].round().astype(int)\n",
    "print(f\"Category_Clean distribution:\\n{df['Category_Clean'].value_counts()}\")\n",
    "print(f\"Purchase rate by Category:\\n{df.groupby('Category_Clean')['Purchase'].mean()}\")\n",
    "\n",
    "# Drop original messy columns\n",
    "df = df.drop(['Email_Interaction', 'Category'], axis=1)\n",
    "\n",
    "# Save cleaned version\n",
    "df.to_csv('df_imputed_fixed.csv')\n",
    "print(\"\\n‚úÖ Saved: df_imputed_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341b0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features: 50\n",
      "Samples: 13735\n",
      "\n",
      "üìä BASELINE PERFORMANCE\n",
      "Best F1: 0.8117 at threshold 0.43\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.945     0.797     0.865      1736\n",
      "         1.0      0.726     0.921     0.812      1011\n",
      "\n",
      "    accuracy                          0.843      2747\n",
      "   macro avg      0.835     0.859     0.838      2747\n",
      "weighted avg      0.864     0.843     0.845      2747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(['Purchase', 'id', 'Session_ID'], axis=1, errors='ignore')\n",
    "y = df['Purchase']\n",
    "\n",
    "print(f\"\\nFeatures: {X.shape[1]}\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train simple XGBoost\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1.72,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Find best threshold\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "\n",
    "for thresh in np.arange(0.3, 0.7, 0.01):\n",
    "    y_pred = (y_pred_proba >= thresh).astype(int)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"\\nüìä BASELINE PERFORMANCE\")\n",
    "print(f\"Best F1: {best_f1:.4f} at threshold {best_thresh:.2f}\")\n",
    "\n",
    "y_pred_final = (y_pred_proba >= best_thresh).astype(int)\n",
    "print(\"\\n\", classification_report(y_val, y_pred_final, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44db696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Email_Campaign_Interaction distribution:\n",
      "Email_Campaign_Interaction\n",
      "0    11472\n",
      "1     2263\n",
      "Name: count, dtype: int64\n",
      "Purchase rate: 0.494\n",
      "\n",
      "üìä WITH EMAIL√óCAMPAIGN FEATURE\n",
      "Best F1: 0.8175 (vs baseline 0.8117)\n",
      "Improvement: +0.0058\n",
      "‚úÖ Feature helped!\n"
     ]
    }
   ],
   "source": [
    "# Create interaction feature\n",
    "if 'Campaign_Period_true' in df.columns:\n",
    "    df['Email_Campaign_Interaction'] = (\n",
    "        (df['Email_Engaged'] == 1) & \n",
    "        (df['Campaign_Period_true'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"\\nEmail_Campaign_Interaction distribution:\")\n",
    "    print(df['Email_Campaign_Interaction'].value_counts())\n",
    "    print(f\"Purchase rate: {df[df['Email_Campaign_Interaction']==1]['Purchase'].mean():.3f}\")\n",
    "\n",
    "# Retrain with new feature\n",
    "X_enhanced = df.drop(['Purchase', 'id', 'Session_ID'], axis=1, errors='ignore')\n",
    "y = df['Purchase']\n",
    "\n",
    "X_train_enh, X_val_enh, y_train_enh, y_val_enh = train_test_split(\n",
    "    X_enhanced, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model_enh = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=1.72,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_enh.fit(X_train_enh, y_train_enh)\n",
    "\n",
    "y_pred_proba_enh = model_enh.predict_proba(X_val_enh)[:, 1]\n",
    "\n",
    "# Find best threshold\n",
    "best_f1_enh = 0\n",
    "best_thresh_enh = 0.5\n",
    "\n",
    "for thresh in np.arange(0.3, 0.7, 0.01):\n",
    "    y_pred = (y_pred_proba_enh >= thresh).astype(int)\n",
    "    f1 = f1_score(y_val_enh, y_pred)\n",
    "    if f1 > best_f1_enh:\n",
    "        best_f1_enh = f1\n",
    "        best_thresh_enh = thresh\n",
    "\n",
    "print(f\"\\nüìä WITH EMAIL√óCAMPAIGN FEATURE\")\n",
    "print(f\"Best F1: {best_f1_enh:.4f} (vs baseline {best_f1:.4f})\")\n",
    "print(f\"Improvement: {best_f1_enh - best_f1:+.4f}\")\n",
    "\n",
    "if best_f1_enh > best_f1:\n",
    "    print(\"‚úÖ Feature helped!\")\n",
    "else:\n",
    "    print(\"‚ùå Feature didn't help - stick with baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd8946bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TESTING ADDITIONAL VALIDATED FEATURES\n",
      "======================================================================\n",
      "Current features: 53\n",
      "\n",
      "1Ô∏è‚É£ Reviews_Read_Any:\n",
      "   Distribution: {1: 13048, 0: 687}\n",
      "   Purchase rate: 0.378 (baseline: 0.368)\n",
      "   Lift: +2.7%\n",
      "\n",
      "2Ô∏è‚É£ Is_Tablet_User:\n",
      "   Distribution: {0.0: 11730, 1.0: 2005}\n",
      "   Purchase rate: 0.431\n",
      "   Lift: +17.2%\n",
      "\n",
      "3Ô∏è‚É£ High_Engagement_Shopper (Reviews + Cart):\n",
      "   Distribution: {1: 11889, 0: 1846}\n",
      "   Purchase rate: 0.331\n",
      "   Lift: -10.1%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Load fixed data\n",
    "df = pd.read_csv('df_imputed_fixed.csv', index_col=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING ADDITIONAL VALIDATED FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Current features we have\n",
    "print(f\"Current features: {df.shape[1]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Feature 1: Reviews_Read_Binary (+27% lift validated)\n",
    "# ============================================================================\n",
    "df['Reviews_Read_Any'] = (df['Reviews_Read'] > 0).astype(int)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ Reviews_Read_Any:\")\n",
    "print(f\"   Distribution: {df['Reviews_Read_Any'].value_counts().to_dict()}\")\n",
    "read_rate = df[df['Reviews_Read_Any'] == 1]['Purchase'].mean()\n",
    "baseline = df['Purchase'].mean()\n",
    "print(f\"   Purchase rate: {read_rate:.3f} (baseline: {baseline:.3f})\")\n",
    "print(f\"   Lift: {(read_rate/baseline - 1)*100:+.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# Feature 2: Device_Is_Tablet (+17% lift validated)\n",
    "# ============================================================================\n",
    "if 'Device_Type_tablet' in df.columns:\n",
    "    df['Is_Tablet_User'] = df['Device_Type_tablet']\n",
    "    tablet_rate = df[df['Is_Tablet_User'] == 1]['Purchase'].mean()\n",
    "    print(f\"\\n2Ô∏è‚É£ Is_Tablet_User:\")\n",
    "    print(f\"   Distribution: {df['Is_Tablet_User'].value_counts().to_dict()}\")\n",
    "    print(f\"   Purchase rate: {tablet_rate:.3f}\")\n",
    "    print(f\"   Lift: {(tablet_rate/baseline - 1)*100:+.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# Feature 3: High_Engagement_Shopper (Reviews + Items in Cart)\n",
    "# ============================================================================\n",
    "# Combining two signals: reads reviews AND has items in cart\n",
    "df['High_Engagement_Shopper'] = (\n",
    "    (df['Reviews_Read'] > 0) & \n",
    "    (df['Items_In_Cart'] > 0)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ High_Engagement_Shopper (Reviews + Cart):\")\n",
    "print(f\"   Distribution: {df['High_Engagement_Shopper'].value_counts().to_dict()}\")\n",
    "engaged_rate = df[df['High_Engagement_Shopper'] == 1]['Purchase'].mean()\n",
    "print(f\"   Purchase rate: {engaged_rate:.3f}\")\n",
    "print(f\"   Lift: {(engaged_rate/baseline - 1)*100:+.1f}%\")\n",
    "\n",
    "# ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c0e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING WITH ONLY GOOD FEATURES\n",
      "======================================================================\n",
      "‚úÖ Added 1 engineered feature: Email_Campaign_Interaction\n",
      "‚úÖ Using existing feature: Device_Type_tablet (+17% lift)\n",
      "\n",
      "Total features: 51\n",
      "\n",
      "üìä PERFORMANCE\n",
      "Best F1: 0.8178 at threshold 0.49\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.930     0.831     0.878      1736\n",
      "         1.0      0.755     0.892     0.818      1011\n",
      "\n",
      "    accuracy                          0.854      2747\n",
      "   macro avg      0.842     0.862     0.848      2747\n",
      "weighted avg      0.865     0.854     0.856      2747\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TOP 20 FEATURES\n",
      "======================================================================\n",
      "                       feature  importance\n",
      "                 Items_In_Cart    0.195499\n",
      "                 Email_Engaged    0.047328\n",
      "          Campaign_Period_true    0.039422\n",
      "            Device_Type_mobile    0.035994\n",
      "    Email_Campaign_Interaction    0.035575\n",
      "                  Reviews_Read    0.034864\n",
      "                Category_Clean    0.033629\n",
      "         Campaign_Period_false    0.033613\n",
      "                         Price    0.033124\n",
      "              Engagement_Score    0.026474\n",
      "            Device_Type_tablet    0.024807\n",
      "        PM_RS_Combo_bank:email    0.016991\n",
      "        PM_RS_Combo_cash:email    0.016608\n",
      "      PM_RS_Combo_credit:email    0.016408\n",
      "         Referral_Source_email    0.014969\n",
      " PM_RS_Combo_cash:social_media    0.014719\n",
      "        PM_RS_Combo_paypal:ads    0.014611\n",
      "       PM_RS_Combo_bank:direct    0.014504\n",
      "PM_RS_Combo_cash:search_engine    0.014246\n",
      "                           Day    0.013770\n",
      "\n",
      "üìä Email_Campaign_Interaction:\n",
      "   Importance: 0.035575\n",
      "   Rank: #51 out of 51\n",
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON\n",
      "======================================================================\n",
      "Your original best XGBoost:     F1 = 0.8185\n",
      "Today's baseline:               F1 = 0.8117\n",
      "With Email√óCampaign feature:    F1 = 0.8175\n",
      "Final model (optimized):        F1 = 0.8178\n",
      "\n",
      "‚úÖ Solid performance, close to best\n",
      "\n",
      "üí° RECOMMENDATION: SUBMIT THIS MODEL OR TRY CLUSTERING\n",
      "\n",
      "‚úÖ Saved: xgboost_final.pkl\n",
      "‚úÖ Saved: df_final_features.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('/Users/jakobbullinger/Documents/Coding Projects/DSBA/Intro Machine Learning/kaggle_competition/notebooks/Jakob/df_imputed_fixed.csv', index_col=0)\n",
    "\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING WITH ONLY GOOD FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Add only features that showed positive lift\n",
    "df['Email_Campaign_Interaction'] = (\n",
    "    (df['Email_Engaged'] == 1) & \n",
    "    (df['Campaign_Period_true'] == 1)\n",
    ").astype(int)\n",
    "\n",
    "# Tablet user is already encoded as Device_Type_tablet, no need to add\n",
    "\n",
    "print(f\"‚úÖ Added 1 engineered feature: Email_Campaign_Interaction\")\n",
    "print(f\"‚úÖ Using existing feature: Device_Type_tablet (+17% lift)\")\n",
    "\n",
    "# Prepare data\n",
    "feature_cols = [col for col in df.columns \n",
    "                if col not in ['Purchase', 'id', 'Session_ID']]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Purchase']\n",
    "\n",
    "print(f\"\\nTotal features: {len(feature_cols)}\")\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train with regularization\n",
    "model = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1.72,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Find optimal threshold\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "\n",
    "for thresh in np.arange(0.3, 0.7, 0.01):\n",
    "    y_pred = (y_pred_proba >= thresh).astype(int)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE\")\n",
    "print(f\"Best F1: {best_f1:.4f} at threshold {best_thresh:.2f}\")\n",
    "\n",
    "y_pred_final = (y_pred_proba >= best_thresh).astype(int)\n",
    "print(\"\\n\", classification_report(y_val, y_pred_final, digits=3))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP 20 FEATURES\")\n",
    "print(\"=\"*70)\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Check our engineered feature\n",
    "email_campaign_importance = feature_importance[\n",
    "    feature_importance['feature'] == 'Email_Campaign_Interaction'\n",
    "]\n",
    "if not email_campaign_importance.empty:\n",
    "    rank = feature_importance.index[feature_importance['feature'] == 'Email_Campaign_Interaction'].tolist()[0] + 1\n",
    "    print(f\"\\nüìä Email_Campaign_Interaction:\")\n",
    "    print(f\"   Importance: {email_campaign_importance['importance'].values[0]:.6f}\")\n",
    "    print(f\"   Rank: #{rank} out of {len(feature_cols)}\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Your original best XGBoost:     F1 = 0.8185\")\n",
    "print(f\"Today's baseline:               F1 = 0.8117\")\n",
    "print(f\"With Email√óCampaign feature:    F1 = 0.8175\")\n",
    "print(f\"Final model (optimized):        F1 = {best_f1:.4f}\")\n",
    "\n",
    "if best_f1 >= 0.8185:\n",
    "    print(f\"\\nüéâ NEW BEST! (+{best_f1 - 0.8185:.4f} improvement)\")\n",
    "    decision = \"SUBMIT THIS MODEL\"\n",
    "elif best_f1 >= 0.8175:\n",
    "    print(f\"\\n‚úÖ Solid performance, close to best\")\n",
    "    decision = \"SUBMIT THIS MODEL OR TRY CLUSTERING\"\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Not better than baseline\")\n",
    "    decision = \"REVERT TO ORIGINAL MODEL (0.8185)\"\n",
    "\n",
    "print(f\"\\nüí° RECOMMENDATION: {decision}\")\n",
    "\n",
    "# Save\n",
    "import pickle\n",
    "with open('xgboost_final.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "df.to_csv('df_final_features.csv')\n",
    "print(\"\\n‚úÖ Saved: xgboost_final.pkl\")\n",
    "print(\"‚úÖ Saved: df_final_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b5b9270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ADDING CLUSTERING (PAOLA'S APPROACH) TO YOUR MODEL\n",
      "======================================================================\n",
      "Starting features: 51\n",
      "Train: 10988, Val: 2747\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 1: Training K-Means Clustering\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Created 4 clusters\n",
      "\n",
      "Training cluster distribution:\n",
      "0    4014\n",
      "1    2617\n",
      "2    2138\n",
      "3    2219\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cluster purchase rates (training):\n",
      "         count      mean    lift_%\n",
      "Cluster                           \n",
      "0         4014  0.375436  1.984932\n",
      "1         2617  0.369507  0.374380\n",
      "2         2138  0.357343 -2.929832\n",
      "3         2219  0.363677 -1.209233\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 2: Adding Cluster Features\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Features after adding clusters: 55\n",
      "   Added 4 cluster features\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 3: Training XGBoost with Cluster Features\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä PERFORMANCE WITH CLUSTERS\n",
      "Best F1: 0.8166 at threshold 0.43\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.947     0.804     0.869      1736\n",
      "         1.0      0.732     0.923     0.817      1011\n",
      "\n",
      "    accuracy                          0.847      2747\n",
      "   macro avg      0.840     0.863     0.843      2747\n",
      "weighted avg      0.868     0.847     0.850      2747\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "STEP 4: Feature Importance Analysis\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Top 20 Features:\n",
      "                        feature  importance\n",
      "                  Items_In_Cart    0.185877\n",
      "          Campaign_Period_false    0.047063\n",
      "           Campaign_Period_true    0.046538\n",
      "                  Email_Engaged    0.040622\n",
      "             Device_Type_mobile    0.035070\n",
      "     Email_Campaign_Interaction    0.032004\n",
      "                          Price    0.031637\n",
      "                   Reviews_Read    0.029323\n",
      "                 Category_Clean    0.028912\n",
      "               Engagement_Score    0.025815\n",
      "             Device_Type_tablet    0.023936\n",
      "         PM_RS_Combo_bank:email    0.018022\n",
      "      PM_RS_Combo_paypal:direct    0.016854\n",
      "         PM_RS_Combo_paypal:ads    0.016170\n",
      "        PM_RS_Combo_bank:direct    0.015740\n",
      "          Referral_Source_email    0.015391\n",
      " PM_RS_Combo_cash:search_engine    0.014920\n",
      "PM_RS_Combo_credit:social_media    0.013412\n",
      "         PM_RS_Combo_cash:email    0.013152\n",
      "       PM_RS_Combo_paypal:email    0.012974\n",
      "\n",
      "üìä Cluster Feature Importance:\n",
      "  feature  importance\n",
      "Cluster_2    0.012167\n",
      "Cluster_1    0.009079\n",
      "Cluster_0    0.008122\n",
      "Cluster_3    0.006917\n",
      "   Total cluster importance: 0.036285\n",
      "\n",
      "======================================================================\n",
      "FINAL DECISION\n",
      "======================================================================\n",
      "Original best XGBoost.............. F1 = 0.8185\n",
      "Without clusters................... F1 = 0.8178\n",
      "WITH clusters...................... F1 = 0.8166\n",
      "\n",
      "Cluster improvement: -0.0012\n",
      "\n",
      "‚ö†Ô∏è Clusters didn't help\n",
      "üí° SUBMIT ORIGINAL 0.8185 MODEL\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ADDING CLUSTERING (PAOLA'S APPROACH) TO YOUR MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load your processed data\n",
    "df = pd.read_csv('df_final_features.csv', index_col=0)\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(['Purchase', 'id', 'Session_ID'], axis=1, errors='ignore')\n",
    "y = df['Purchase']\n",
    "\n",
    "print(f\"Starting features: {X.shape[1]}\")\n",
    "\n",
    "# Split first (important: cluster on training data only)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Create Clusters (K-Means on Training Data)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 1: Training K-Means Clustering\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Scale features for clustering\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Train K-Means (using Paola's n_clusters=4)\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(X_train_scaled)\n",
    "\n",
    "# Assign clusters\n",
    "train_clusters = kmeans.labels_\n",
    "val_clusters = kmeans.predict(X_val_scaled)\n",
    "\n",
    "print(f\"‚úÖ Created {n_clusters} clusters\")\n",
    "print(f\"\\nTraining cluster distribution:\")\n",
    "print(pd.Series(train_clusters).value_counts().sort_index())\n",
    "\n",
    "# Analyze clusters\n",
    "print(f\"\\nCluster purchase rates (training):\")\n",
    "cluster_analysis = pd.DataFrame({\n",
    "    'Cluster': train_clusters,\n",
    "    'Purchase': y_train\n",
    "})\n",
    "cluster_rates = cluster_analysis.groupby('Cluster')['Purchase'].agg(['count', 'mean'])\n",
    "cluster_rates['lift_%'] = (cluster_rates['mean'] / y_train.mean() - 1) * 100\n",
    "print(cluster_rates)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Add Cluster as Features\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 2: Adding Cluster Features\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Add cluster as categorical feature (one-hot encode)\n",
    "X_train_with_cluster = X_train.copy()\n",
    "X_val_with_cluster = X_val.copy()\n",
    "\n",
    "X_train_with_cluster['Cluster'] = train_clusters\n",
    "X_val_with_cluster['Cluster'] = val_clusters\n",
    "\n",
    "# One-hot encode cluster (create dummy variables)\n",
    "X_train_final = pd.get_dummies(X_train_with_cluster, columns=['Cluster'], prefix='Cluster')\n",
    "X_val_final = pd.get_dummies(X_val_with_cluster, columns=['Cluster'], prefix='Cluster')\n",
    "\n",
    "# Align columns (in case validation is missing a cluster)\n",
    "for col in X_train_final.columns:\n",
    "    if col not in X_val_final.columns:\n",
    "        X_val_final[col] = 0\n",
    "\n",
    "X_val_final = X_val_final[X_train_final.columns]\n",
    "\n",
    "print(f\"‚úÖ Features after adding clusters: {X_train_final.shape[1]}\")\n",
    "print(f\"   Added {n_clusters} cluster features\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Train XGBoost with Clusters\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 3: Training XGBoost with Cluster Features\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "model_with_clusters = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    min_child_weight=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1.72,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_with_clusters.fit(X_train_final, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_proba = model_with_clusters.predict_proba(X_val_final)[:, 1]\n",
    "\n",
    "# Find optimal threshold\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "\n",
    "for thresh in np.arange(0.3, 0.7, 0.01):\n",
    "    y_pred = (y_pred_proba >= thresh).astype(int)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE WITH CLUSTERS\")\n",
    "print(f\"Best F1: {best_f1:.4f} at threshold {best_thresh:.2f}\")\n",
    "\n",
    "y_pred_final = (y_pred_proba >= best_thresh).astype(int)\n",
    "print(\"\\n\", classification_report(y_val, y_pred_final, digits=3))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Feature Importance\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"STEP 4: Feature Importance Analysis\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_final.columns,\n",
    "    'importance': model_with_clusters.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features:\")\n",
    "print(feature_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Check cluster feature importance\n",
    "cluster_features = [col for col in feature_importance['feature'] if 'Cluster_' in col]\n",
    "cluster_importance_df = feature_importance[feature_importance['feature'].isin(cluster_features)]\n",
    "\n",
    "if not cluster_importance_df.empty:\n",
    "    print(f\"\\nüìä Cluster Feature Importance:\")\n",
    "    print(cluster_importance_df.to_string(index=False))\n",
    "    total_cluster_importance = cluster_importance_df['importance'].sum()\n",
    "    print(f\"   Total cluster importance: {total_cluster_importance:.6f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No cluster features in importance (might be too weak)\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DECISION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = {\n",
    "    'Original best XGBoost': 0.8185,\n",
    "    'Without clusters': 0.8178,\n",
    "    'WITH clusters': best_f1\n",
    "}\n",
    "\n",
    "for name, score in results.items():\n",
    "    print(f\"{name:.<35} F1 = {score:.4f}\")\n",
    "\n",
    "improvement = best_f1 - 0.8178\n",
    "print(f\"\\nCluster improvement: {improvement:+.4f}\")\n",
    "\n",
    "if best_f1 > 0.8185:\n",
    "    print(\"\\nüéâ NEW BEST MODEL! Clusters helped!\")\n",
    "    print(\"‚úÖ SUBMIT THIS MODEL\")\n",
    "    best_model_to_use = model_with_clusters\n",
    "    best_data_to_use = (X_train_final, X_val_final)\n",
    "elif best_f1 >= 0.8178:\n",
    "    print(\"\\n‚úÖ Clusters helped slightly or maintained performance\")\n",
    "    print(\"üí° SUBMIT MODEL WITH CLUSTERS (more interpretable)\")\n",
    "    best_model_to_use = model_with_clusters\n",
    "    best_data_to_use = (X_train_final, X_val_final)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Clusters didn't help\")\n",
    "    print(\"üí° SUBMIT ORIGINAL 0.8185 MODEL\")\n",
    "    best_model_to_use = None\n",
    "\n",
    "# Save if improved\n",
    "if best_f1 >= 0.8178:\n",
    "    import pickle\n",
    "    with open('xgboost_with_clusters.pkl', 'wb') as f:\n",
    "        pickle.dump(model_with_clusters, f)\n",
    "    with open('kmeans_model.pkl', 'wb') as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    with open('scaler_for_kmeans.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(\"\\n‚úÖ Saved: xgboost_with_clusters.pkl\")\n",
    "    print(\"‚úÖ Saved: kmeans_model.pkl\")\n",
    "    print(\"‚úÖ Saved: scaler_for_kmeans.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kaggle_venv)",
   "language": "python",
   "name": "kaggle_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
