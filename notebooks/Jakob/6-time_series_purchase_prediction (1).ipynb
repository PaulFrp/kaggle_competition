{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Modeling for Purchase Prediction\n",
    "\n",
    "This notebook implements time series modeling approaches to predict purchases, leveraging temporal features and respecting the temporal nature of the data (Days 1-70).\n",
    "\n",
    "## Key Objectives:\n",
    "- Leverage temporal features (day_sin, day_cos, time_sin, time_cos)\n",
    "- Implement proper time-based validation\n",
    "- Optimize for F1 score\n",
    "- Compare different modeling approaches suitable for time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('/Users/jakobbullinger/Documents/Coding Projects/DSBA/Intro Machine Learning/kaggle_competition/data/imputed/df_imputed.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nNumber of unique days: {df['Day'].nunique()}\")\n",
    "print(f\"Day range: {df['Day'].min()} to {df['Day'].max()}\")\n",
    "print(f\"\\nPurchase rate: {df['Purchase'].mean():.2%}\")\n",
    "print(f\"\\nNumber of sessions: {df['Session_ID'].nunique()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Temporal Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze purchase patterns over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Daily purchase rate\n",
    "daily_stats = df.groupby('Day').agg({\n",
    "    'Purchase': ['mean', 'sum', 'count']\n",
    "}).round(3)\n",
    "daily_stats.columns = ['purchase_rate', 'total_purchases', 'total_sessions']\n",
    "\n",
    "axes[0, 0].plot(daily_stats.index, daily_stats['purchase_rate'], marker='o', markersize=3)\n",
    "axes[0, 0].set_title('Daily Purchase Rate Over Time')\n",
    "axes[0, 0].set_xlabel('Day')\n",
    "axes[0, 0].set_ylabel('Purchase Rate')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume over time\n",
    "axes[0, 1].bar(daily_stats.index, daily_stats['total_sessions'], alpha=0.7, label='Sessions')\n",
    "axes[0, 1].bar(daily_stats.index, daily_stats['total_purchases'], alpha=0.7, label='Purchases')\n",
    "axes[0, 1].set_title('Daily Volume')\n",
    "axes[0, 1].set_xlabel('Day')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Weekly patterns\n",
    "df['Week'] = (df['Day'] - 1) // 7 + 1\n",
    "weekly_stats = df.groupby('Week')['Purchase'].mean()\n",
    "axes[1, 0].bar(weekly_stats.index, weekly_stats.values)\n",
    "axes[1, 0].set_title('Weekly Purchase Rate')\n",
    "axes[1, 0].set_xlabel('Week')\n",
    "axes[1, 0].set_ylabel('Purchase Rate')\n",
    "\n",
    "# Campaign effect over time\n",
    "campaign_daily = df.groupby(['Day', 'Campaign_Period_true'])['Purchase'].mean().unstack(fill_value=0)\n",
    "axes[1, 1].plot(campaign_daily.index, campaign_daily[0], label='No Campaign', marker='o', markersize=3)\n",
    "axes[1, 1].plot(campaign_daily.index, campaign_daily[1], label='Campaign', marker='s', markersize=3)\n",
    "axes[1, 1].set_title('Purchase Rate by Campaign Status')\n",
    "axes[1, 1].set_xlabel('Day')\n",
    "axes[1, 1].set_ylabel('Purchase Rate')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary Statistics by Week:\")\n",
    "print(df.groupby('Week')['Purchase'].agg(['mean', 'std', 'count']).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional time-based features\n",
    "print(\"Creating additional temporal features...\")\n",
    "\n",
    "# Rolling statistics (looking back only - no data leakage)\n",
    "def create_lag_features(df, window_sizes=[3, 7, 14]):\n",
    "    df_sorted = df.sort_values('Day').copy()\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        # Calculate rolling statistics for each day (excluding current day)\n",
    "        daily_rates = df_sorted.groupby('Day')['Purchase'].mean().shift(1)\n",
    "        rolling_mean = daily_rates.rolling(window=window, min_periods=1).mean()\n",
    "        rolling_std = daily_rates.rolling(window=window, min_periods=1).std().fillna(0)\n",
    "        \n",
    "        # Map back to original dataframe\n",
    "        df_sorted[f'purchase_rate_ma{window}'] = df_sorted['Day'].map(rolling_mean)\n",
    "        df_sorted[f'purchase_rate_std{window}'] = df_sorted['Day'].map(rolling_std)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "# Apply lag features\n",
    "df = create_lag_features(df)\n",
    "\n",
    "# Day of week features (cyclical)\n",
    "df['day_of_week'] = df['Day'] % 7\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "# Trend features\n",
    "df['day_normalized'] = df['Day'] / df['Day'].max()  # Normalized day for trend\n",
    "df['day_squared'] = df['day_normalized'] ** 2  # Quadratic trend\n",
    "\n",
    "# Interaction between time and campaign\n",
    "df['campaign_day_interaction'] = df['Campaign_Period_true'] * df['Day']\n",
    "df['campaign_time_sin'] = df['Campaign_Period_true'] * df['time_sin']\n",
    "df['campaign_time_cos'] = df['Campaign_Period_true'] * df['time_cos']\n",
    "\n",
    "# Email interaction patterns over time\n",
    "df['email_day_interaction'] = df['Email_Interaction'] * df['day_normalized']\n",
    "\n",
    "# Create momentum features (purchase velocity)\n",
    "daily_velocity = df.groupby('Day')['Purchase'].mean().diff().shift(1)\n",
    "df['purchase_velocity'] = df['Day'].map(daily_velocity).fillna(0)\n",
    "\n",
    "print(f\"Total features after engineering: {len([col for col in df.columns if col not in ['id', 'Session_ID', 'Purchase']])}\")\n",
    "print(\"\\nNew temporal features created:\")\n",
    "new_features = [col for col in df.columns if any(x in col for x in ['ma', 'std', 'velocity', 'interaction', 'normalized', 'squared'])]\n",
    "print(new_features[:10], '...') if len(new_features) > 10 else print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time-Based Train-Test Split\n",
    "\n",
    "For time series, we need to respect temporal ordering. We'll use the last 20% of days as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split point\n",
    "split_day = int(df['Day'].max() * 0.8)  # Day 56 for 70 days\n",
    "print(f\"Training on days 1-{split_day}, Testing on days {split_day+1}-{df['Day'].max()}\")\n",
    "\n",
    "# Create train and test sets\n",
    "train_df = df[df['Day'] <= split_day].copy()\n",
    "test_df = df[df['Day'] > split_day].copy()\n",
    "\n",
    "print(f\"\\nTrain set: {train_df.shape[0]} samples (Days 1-{split_day})\")\n",
    "print(f\"Test set: {test_df.shape[0]} samples (Days {split_day+1}-{df['Day'].max()})\")\n",
    "print(f\"\\nTrain purchase rate: {train_df['Purchase'].mean():.2%}\")\n",
    "print(f\"Test purchase rate: {test_df['Purchase'].mean():.2%}\")\n",
    "\n",
    "# Prepare features and target\n",
    "exclude_cols = ['id', 'Session_ID', 'Purchase', 'Day', 'Week']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['Purchase']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['Purchase']\n",
    "\n",
    "# Clean feature names for LightGBM (remove special characters)\n",
    "print(\"\\nCleaning feature names for LightGBM compatibility...\")\n",
    "clean_feature_names = {}\n",
    "for col in X_train.columns:\n",
    "    # Replace special characters with underscores\n",
    "    clean_name = col.replace(':', '_').replace('[', '_').replace(']', '_').replace(',', '_').replace(' ', '_')\n",
    "    clean_feature_names[col] = clean_name\n",
    "\n",
    "# Rename columns\n",
    "X_train = X_train.rename(columns=clean_feature_names)\n",
    "X_test = X_test.rename(columns=clean_feature_names)\n",
    "\n",
    "print(f\"\\nNumber of features: {X_train.shape[1]}\")\n",
    "print(f\"Features shape - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Example cleaned feature names: {list(X_train.columns[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development and Comparison\n",
    "\n",
    "We'll compare multiple approaches:\n",
    "1. LightGBM (strong baseline from previous work)\n",
    "2. XGBoost with temporal features\n",
    "3. Time-aware ensemble\n",
    "4. Recency-weighted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for model evaluation\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with focus on F1 score\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Apply threshold\n",
    "    y_train_pred = (y_train_pred_proba >= threshold).astype(int)\n",
    "    y_test_pred = (y_test_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'threshold': threshold,\n",
    "        'train_f1': f1_score(y_train, y_train_pred),\n",
    "        'test_f1': f1_score(y_test, y_test_pred),\n",
    "        'train_precision': precision_score(y_train, y_train_pred),\n",
    "        'test_precision': precision_score(y_test, y_test_pred),\n",
    "        'train_recall': recall_score(y_train, y_train_pred),\n",
    "        'test_recall': recall_score(y_test, y_test_pred),\n",
    "        'train_auc': roc_auc_score(y_train, y_train_pred_proba),\n",
    "        'test_auc': roc_auc_score(y_test, y_test_pred_proba),\n",
    "        'test_positive_rate': y_test_pred.mean()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Threshold: {threshold:.2f}\")\n",
    "    print(f\"\\nF1 Score - Train: {results['train_f1']:.4f}, Test: {results['test_f1']:.4f}\")\n",
    "    print(f\"Precision - Train: {results['train_precision']:.4f}, Test: {results['test_precision']:.4f}\")\n",
    "    print(f\"Recall - Train: {results['train_recall']:.4f}, Test: {results['test_recall']:.4f}\")\n",
    "    print(f\"AUC - Train: {results['train_auc']:.4f}, Test: {results['test_auc']:.4f}\")\n",
    "    print(f\"\\nTest Set Positive Rate: {results['test_positive_rate']:.2%}\")\n",
    "    \n",
    "    return results, y_test_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: LightGBM with temporal features\n",
    "print(\"Training LightGBM with temporal features...\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'random_state': 42,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "# Create dataset\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_val = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# Train with early stopping\n",
    "lgb_model = lgb.train(\n",
    "    lgb_params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_val],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    ")\n",
    "\n",
    "# Create sklearn-compatible wrapper for evaluation\n",
    "class LGBWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        proba = self.model.predict(X, num_iteration=self.model.best_iteration)\n",
    "        return np.vstack([1 - proba, proba]).T\n",
    "\n",
    "lgb_wrapper = LGBWrapper(lgb_model)\n",
    "lgb_results, lgb_proba = evaluate_model(lgb_wrapper, X_train, y_train, X_test, y_test, \"LightGBM\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: XGBoost with sample weights (recent data weighted more)\n",
    "print(\"\\nTraining XGBoost with recency weighting...\")\n",
    "\n",
    "# Create sample weights - more recent samples get higher weight\n",
    "train_weights = np.exp((train_df['Day'] - train_df['Day'].min()) / 30)  # Exponential decay\n",
    "train_weights = train_weights / train_weights.mean()  # Normalize\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    early_stopping_rounds=50\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=train_weights,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_results, xgb_proba = evaluate_model(xgb_model, X_train, y_train, X_test, y_test, \"XGBoost (Weighted)\", 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: LightGBM with Time Series Cross-Validation for hyperparameter tuning\n",
    "print(\"\\nTraining LightGBM with TimeSeriesSplit optimization...\")\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define parameter space\n",
    "param_dist = {\n",
    "    'num_leaves': randint(20, 50),\n",
    "    'max_depth': randint(4, 10),\n",
    "    'learning_rate': uniform(0.01, 0.15),\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'min_child_samples': randint(10, 30),\n",
    "    'subsample': uniform(0.6, 0.3),\n",
    "    'colsample_bytree': uniform(0.6, 0.3),\n",
    "    'reg_alpha': uniform(0, 1),\n",
    "    'reg_lambda': uniform(0, 1)\n",
    "}\n",
    "\n",
    "# Time series split for validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Base model\n",
    "lgb_ts = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    metric='binary_logloss',\n",
    "    random_state=42,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "# Random search with time series cross-validation\n",
    "print(\"Performing randomized search with TimeSeriesSplit...\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    lgb_ts,\n",
    "    param_dist,\n",
    "    n_iter=20,\n",
    "    cv=tscv,\n",
    "    scoring='f1',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best F1 score from CV: {random_search.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "\n",
    "lgb_ts_results, lgb_ts_proba = evaluate_model(random_search.best_estimator_, X_train, y_train, X_test, y_test, \"LightGBM (TS-Optimized)\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Threshold Optimization for F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(y_true, y_proba, metric='f1'):\n",
    "    \"\"\"\n",
    "    Find optimal threshold for classification\n",
    "    \"\"\"\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        if metric == 'f1':\n",
    "            score = f1_score(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            score = precision_score(y_true, y_pred)\n",
    "        elif metric == 'recall':\n",
    "            score = recall_score(y_true, y_pred)\n",
    "        scores.append(score)\n",
    "    \n",
    "    optimal_idx = np.argmax(scores)\n",
    "    return thresholds[optimal_idx], scores[optimal_idx]\n",
    "\n",
    "# Find optimal thresholds for each model\n",
    "print(\"Finding optimal thresholds for each model...\\n\")\n",
    "\n",
    "models_proba = {\n",
    "    'LightGBM': lgb_proba,\n",
    "    'XGBoost (Weighted)': xgb_proba,\n",
    "    'LightGBM (TS-Optimized)': lgb_ts_proba\n",
    "}\n",
    "\n",
    "optimal_thresholds = {}\n",
    "for model_name, proba in models_proba.items():\n",
    "    opt_threshold, opt_score = find_optimal_threshold(y_test, proba, 'f1')\n",
    "    optimal_thresholds[model_name] = opt_threshold\n",
    "    \n",
    "    # Re-evaluate with optimal threshold\n",
    "    y_pred_opt = (proba >= opt_threshold).astype(int)\n",
    "    f1_opt = f1_score(y_test, y_pred_opt)\n",
    "    precision_opt = precision_score(y_test, y_pred_opt)\n",
    "    recall_opt = recall_score(y_test, y_pred_opt)\n",
    "    positive_rate = y_pred_opt.mean()\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Optimal Threshold: {opt_threshold:.3f}\")\n",
    "    print(f\"  F1 Score: {f1_opt:.4f}\")\n",
    "    print(f\"  Precision: {precision_opt:.4f}\")\n",
    "    print(f\"  Recall: {recall_opt:.4f}\")\n",
    "    print(f\"  Positive Rate: {positive_rate:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from best model\n",
    "feature_importance = lgb_model.feature_importance(importance_type='gain')\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Separate temporal vs non-temporal features\n",
    "temporal_keywords = ['day', 'time', 'ma', 'std', 'velocity', 'week', 'campaign_day', 'campaign_time', 'email_day']\n",
    "importance_df['is_temporal'] = importance_df['feature'].apply(\n",
    "    lambda x: any(keyword in x.lower() for keyword in temporal_keywords)\n",
    ")\n",
    "\n",
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top 20 overall features\n",
    "top_20 = importance_df.head(20)\n",
    "colors = ['coral' if x else 'skyblue' for x in top_20['is_temporal']]\n",
    "axes[0].barh(range(len(top_20)), top_20['importance'], color=colors)\n",
    "axes[0].set_yticks(range(len(top_20)))\n",
    "axes[0].set_yticklabels(top_20['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('Top 20 Features (Red=Temporal, Blue=Non-temporal)')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Temporal features only\n",
    "temporal_features = importance_df[importance_df['is_temporal']].head(15)\n",
    "axes[1].barh(range(len(temporal_features)), temporal_features['importance'], color='coral')\n",
    "axes[1].set_yticks(range(len(temporal_features)))\n",
    "axes[1].set_yticklabels(temporal_features['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Top Temporal Features')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTemporal Features Contribution:\")\n",
    "temporal_importance = importance_df[importance_df['is_temporal']]['importance'].sum()\n",
    "total_importance = importance_df['importance'].sum()\n",
    "print(f\"Temporal features account for {temporal_importance/total_importance:.1%} of total importance\")\n",
    "print(f\"\\nTop 5 temporal features:\")\n",
    "for _, row in temporal_features.head(5).iterrows():\n",
    "    print(f\"  - {row['feature']}: {row['importance']:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time-Based Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model performance over time in test set\n",
    "best_model_proba = lgb_ts_proba\n",
    "best_threshold = optimal_thresholds['LightGBM (TS-Optimized)']\n",
    "\n",
    "# Add predictions to test dataframe\n",
    "test_df_analysis = test_df.copy()\n",
    "test_df_analysis['predicted_proba'] = best_model_proba\n",
    "test_df_analysis['predicted'] = (best_model_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Calculate daily metrics\n",
    "daily_metrics = test_df_analysis.groupby('Day').apply(lambda x: pd.Series({\n",
    "    'f1': f1_score(x['Purchase'], x['predicted']) if x['Purchase'].sum() > 0 else 0,\n",
    "    'precision': precision_score(x['Purchase'], x['predicted'], zero_division=0),\n",
    "    'recall': recall_score(x['Purchase'], x['predicted'], zero_division=0),\n",
    "    'actual_rate': x['Purchase'].mean(),\n",
    "    'predicted_rate': x['predicted'].mean(),\n",
    "    'n_samples': len(x)\n",
    "}))\n",
    "\n",
    "# Visualize performance over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# F1 score over time\n",
    "axes[0, 0].plot(daily_metrics.index, daily_metrics['f1'], marker='o', label='F1 Score')\n",
    "axes[0, 0].axhline(y=daily_metrics['f1'].mean(), color='r', linestyle='--', label=f'Mean: {daily_metrics[\"f1\"].mean():.3f}')\n",
    "axes[0, 0].set_xlabel('Day')\n",
    "axes[0, 0].set_ylabel('F1 Score')\n",
    "axes[0, 0].set_title('Daily F1 Score in Test Period')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision and Recall\n",
    "axes[0, 1].plot(daily_metrics.index, daily_metrics['precision'], marker='s', label='Precision')\n",
    "axes[0, 1].plot(daily_metrics.index, daily_metrics['recall'], marker='^', label='Recall')\n",
    "axes[0, 1].set_xlabel('Day')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_title('Daily Precision and Recall')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted rates\n",
    "axes[1, 0].plot(daily_metrics.index, daily_metrics['actual_rate'], marker='o', label='Actual Rate')\n",
    "axes[1, 0].plot(daily_metrics.index, daily_metrics['predicted_rate'], marker='s', label='Predicted Rate')\n",
    "axes[1, 0].set_xlabel('Day')\n",
    "axes[1, 0].set_ylabel('Purchase Rate')\n",
    "axes[1, 0].set_title('Actual vs Predicted Purchase Rates')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample volume\n",
    "axes[1, 1].bar(daily_metrics.index, daily_metrics['n_samples'], alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Day')\n",
    "axes[1, 1].set_ylabel('Number of Samples')\n",
    "axes[1, 1].set_title('Daily Sample Volume in Test Period')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Test Period Performance Summary:\")\n",
    "print(f\"Average F1 Score: {daily_metrics['f1'].mean():.4f} (±{daily_metrics['f1'].std():.4f})\")\n",
    "print(f\"Best Day F1: {daily_metrics['f1'].max():.4f} (Day {daily_metrics['f1'].idxmax()})\")\n",
    "print(f\"Worst Day F1: {daily_metrics['f1'].min():.4f} (Day {daily_metrics['f1'].idxmin()})\")\n",
    "print(f\"\\nModel stability (CV of F1): {daily_metrics['f1'].std() / daily_metrics['f1'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble Approach with Time Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble that weights recent predictions more\n",
    "print(\"Creating time-weighted ensemble...\")\n",
    "\n",
    "# Get predictions from all models\n",
    "all_predictions = pd.DataFrame({\n",
    "    'lgb': lgb_proba,\n",
    "    'xgb': xgb_proba,\n",
    "    'lgb_ts': lgb_ts_proba\n",
    "})\n",
    "\n",
    "# Simple average ensemble\n",
    "ensemble_avg = all_predictions.mean(axis=1)\n",
    "\n",
    "# Weighted ensemble (based on individual F1 scores)\n",
    "weights = {\n",
    "    'lgb': lgb_results['test_f1'],\n",
    "    'xgb': xgb_results['test_f1'],\n",
    "    'lgb_ts': lgb_ts_results['test_f1']\n",
    "}\n",
    "total_weight = sum(weights.values())\n",
    "weights = {k: v/total_weight for k, v in weights.items()}\n",
    "\n",
    "ensemble_weighted = (\n",
    "    all_predictions['lgb'] * weights['lgb'] +\n",
    "    all_predictions['xgb'] * weights['xgb'] +\n",
    "    all_predictions['lgb_ts'] * weights['lgb_ts']\n",
    ")\n",
    "\n",
    "# Evaluate ensemble approaches\n",
    "print(\"\\nEnsemble Results:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for ensemble_name, ensemble_proba in [('Average Ensemble', ensemble_avg), ('Weighted Ensemble', ensemble_weighted)]:\n",
    "    opt_threshold, opt_score = find_optimal_threshold(y_test, ensemble_proba, 'f1')\n",
    "    y_pred = (ensemble_proba >= opt_threshold).astype(int)\n",
    "    \n",
    "    print(f\"\\n{ensemble_name}:\")\n",
    "    print(f\"  Optimal Threshold: {opt_threshold:.3f}\")\n",
    "    print(f\"  F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"  Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"  Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"  AUC: {roc_auc_score(y_test, ensemble_proba):.4f}\")\n",
    "    print(f\"  Positive Rate: {y_pred.mean():.2%}\")\n",
    "\n",
    "print(\"\\nModel Weights in Weighted Ensemble:\")\n",
    "for model, weight in weights.items():\n",
    "    print(f\"  {model}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model Selection and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "results_summary = pd.DataFrame([\n",
    "    lgb_results,\n",
    "    xgb_results,\n",
    "    lgb_ts_results\n",
    "])\n",
    "\n",
    "# Add optimized thresholds results\n",
    "for model_name in models_proba.keys():\n",
    "    proba = models_proba[model_name]\n",
    "    threshold = optimal_thresholds[model_name]\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    \n",
    "    results_summary = pd.concat([results_summary, pd.DataFrame([{\n",
    "        'model': f\"{model_name} (Optimized)\",\n",
    "        'threshold': threshold,\n",
    "        'test_f1': f1_score(y_test, y_pred),\n",
    "        'test_precision': precision_score(y_test, y_pred),\n",
    "        'test_recall': recall_score(y_test, y_pred),\n",
    "        'test_auc': roc_auc_score(y_test, proba),\n",
    "        'test_positive_rate': y_pred.mean()\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "# Sort by F1 score\n",
    "results_summary = results_summary.sort_values('test_f1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAll Models Ranked by Test F1 Score:\")\n",
    "print(results_summary[['model', 'threshold', 'test_f1', 'test_precision', 'test_recall', 'test_positive_rate']].to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_model_name = results_summary.iloc[0]['model']\n",
    "best_f1 = results_summary.iloc[0]['test_f1']\n",
    "best_threshold = results_summary.iloc[0]['threshold']\n",
    "best_positive_rate = results_summary.iloc[0]['test_positive_rate']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"F1 Score: {best_f1:.4f}\")\n",
    "print(f\"Optimal Threshold: {best_threshold:.3f}\")\n",
    "print(f\"Positive Rate: {best_positive_rate:.2%}\")\n",
    "\n",
    "# Marketing budget constraint check\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSINESS CONSTRAINT CHECK\")\n",
    "print(\"=\"*80)\n",
    "budget_reduction_target = 200 / 630  # Target: €200 from €630\n",
    "print(f\"Target spending: {budget_reduction_target:.1%} of original (€200/€630)\")\n",
    "print(f\"Model targets: {best_positive_rate:.1%} of users\")\n",
    "if best_positive_rate <= budget_reduction_target + 0.05:  # Allow 5% margin\n",
    "    print(\"✓ Model meets budget constraint!\")\n",
    "else:\n",
    "    print(f\"⚠ Model exceeds budget by {(best_positive_rate - budget_reduction_target)*100:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Recommendations\n",
    "\n",
    "### Time Series Modeling Insights:\n",
    "1. **Temporal Features Impact**: Analyze which temporal features contributed most\n",
    "2. **Model Stability**: How consistent is performance across different time periods?\n",
    "3. **Recency Weighting**: Did giving more weight to recent data improve performance?\n",
    "\n",
    "### Next Steps:\n",
    "1. **Advanced Time Series Models**: Consider LSTM or Prophet for capturing complex temporal patterns\n",
    "2. **Feature Engineering**: Create more sophisticated lag features or rolling statistics\n",
    "3. **Online Learning**: Implement models that can adapt to changing patterns over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final insights\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compare temporal vs non-temporal features\n",
    "temporal_cols = [col for col in X_train.columns if any(k in col.lower() for k in temporal_keywords)]\n",
    "non_temporal_cols = [col for col in X_train.columns if col not in temporal_cols]\n",
    "\n",
    "print(f\"\\n1. Feature Distribution:\")\n",
    "print(f\"   - Total features: {len(X_train.columns)}\")\n",
    "print(f\"   - Temporal features: {len(temporal_cols)} ({len(temporal_cols)/len(X_train.columns):.1%})\")\n",
    "print(f\"   - Non-temporal features: {len(non_temporal_cols)} ({len(non_temporal_cols)/len(X_train.columns):.1%})\")\n",
    "\n",
    "print(f\"\\n2. Model Performance Comparison:\")\n",
    "baseline_f1 = lgb_results['test_f1']\n",
    "ts_optimized_f1 = lgb_ts_results['test_f1']\n",
    "improvement = (ts_optimized_f1 - baseline_f1) / baseline_f1 * 100\n",
    "print(f\"   - Baseline LightGBM F1: {baseline_f1:.4f}\")\n",
    "print(f\"   - TS-Optimized LightGBM F1: {ts_optimized_f1:.4f}\")\n",
    "print(f\"   - Improvement: {improvement:+.1f}%\")\n",
    "\n",
    "print(f\"\\n3. Temporal Stability:\")\n",
    "print(f\"   - F1 Score Std Dev across days: {daily_metrics['f1'].std():.4f}\")\n",
    "print(f\"   - Coefficient of Variation: {daily_metrics['f1'].std() / daily_metrics['f1'].mean():.3f}\")\n",
    "\n",
    "print(f\"\\n4. Business Impact:\")\n",
    "print(f\"   - Optimal threshold found: {best_threshold:.3f}\")\n",
    "print(f\"   - Users targeted: {best_positive_rate:.1%}\")\n",
    "print(f\"   - Expected F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Time series modeling complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
