{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElectroShop Purchase Prediction - EDA & Feature Engineering\n",
    "\n",
    "**Team Member:** [Your Name]\n",
    "\n",
    "**Date:** November 13, 2025\n",
    "\n",
    "## Objective\n",
    "- Explore the ElectroShop dataset\n",
    "- Understand purchase patterns\n",
    "- Create meaningful features to improve prediction\n",
    "- Reduce marketing costs from €630/day to ≤€200/day while maintaining sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stats\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33m../data/train.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m test_df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../data/test.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining set shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and missing values\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = train_df.isnull().sum()\n",
    "missing_pct = 100 * missing / len(train_df)\n",
    "missing_df = pd.DataFrame({'Missing_Count': missing, 'Percentage': missing_pct})\n",
    "missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = train_df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "print(f\"Number of unique Session_IDs: {train_df['Session_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchase rate\n",
    "purchase_rate = train_df['Purchase'].mean()\n",
    "print(f\"Overall purchase rate: {purchase_rate:.2%}\")\n",
    "print(f\"\\nPurchase distribution:\")\n",
    "print(train_df['Purchase'].value_counts())\n",
    "print(f\"\\nPurchase distribution (%):\")\n",
    "print(train_df['Purchase'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "train_df['Purchase'].value_counts().plot(kind='bar', ax=axes[0], color=['#e74c3c', '#2ecc71'])\n",
    "axes[0].set_title('Purchase Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Purchase (0=No, 1=Yes)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['No Purchase', 'Purchase'], rotation=0)\n",
    "\n",
    "# Percentage plot\n",
    "(train_df['Purchase'].value_counts(normalize=True) * 100).plot(kind='bar', ax=axes[1], color=['#e74c3c', '#2ecc71'])\n",
    "axes[1].set_title('Purchase Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Purchase (0=No, 1=Yes)')\n",
    "axes[1].set_ylabel('Percentage (%)')\n",
    "axes[1].set_xticklabels(['No Purchase', 'Purchase'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n⚠️ Class imbalance: {(1 - purchase_rate) / purchase_rate:.1f}:1 ratio (No Purchase : Purchase)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Campaign Period Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchase rate by campaign period\n",
    "campaign_purchase = train_df.groupby('Campaign_Period')['Purchase'].agg(['mean', 'count'])\n",
    "campaign_purchase.columns = ['Purchase_Rate', 'Count']\n",
    "campaign_purchase['Purchase_Rate'] = campaign_purchase['Purchase_Rate'] * 100\n",
    "print(\"Purchase rate by Campaign Period:\")\n",
    "print(campaign_purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchase rate over days\n",
    "daily_stats = train_df.groupby('Day').agg({\n",
    "    'Purchase': ['mean', 'count'],\n",
    "    'Campaign_Period': 'first'\n",
    "}).reset_index()\n",
    "daily_stats.columns = ['Day', 'Purchase_Rate', 'Sessions', 'Campaign_Period']\n",
    "daily_stats['Purchase_Rate'] = daily_stats['Purchase_Rate'] * 100\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# Purchase rate by day\n",
    "colors = ['#e74c3c' if cp else '#3498db' for cp in daily_stats['Campaign_Period']]\n",
    "axes[0].bar(daily_stats['Day'], daily_stats['Purchase_Rate'], color=colors, alpha=0.7)\n",
    "axes[0].axvspan(25, 50, alpha=0.2, color='red', label='Campaign 1')\n",
    "axes[0].axvspan(75, 90, alpha=0.2, color='red', label='Campaign 2')\n",
    "axes[0].set_title('Purchase Rate by Day (Campaign Periods Highlighted)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Day')\n",
    "axes[0].set_ylabel('Purchase Rate (%)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sessions per day\n",
    "axes[1].bar(daily_stats['Day'], daily_stats['Sessions'], color=colors, alpha=0.7)\n",
    "axes[1].axvspan(25, 50, alpha=0.2, color='red')\n",
    "axes[1].axvspan(75, 90, alpha=0.2, color='red')\n",
    "axes[1].set_title('Number of Sessions by Day', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Number of Sessions')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Univariate Analysis - Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical features\n",
    "numerical_features = ['Age', 'Reviews_Read', 'Price', 'Discount', 'Items_In_Cart', \n",
    "                      'Socioeconomic_Status_Score', 'Engagement_Score']\n",
    "\n",
    "# Distribution plots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    axes[idx].hist(train_df[col], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove extra subplots\n",
    "for idx in range(len(numerical_features), 9):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots to identify outliers\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    axes[idx].boxplot(train_df[col].dropna())\n",
    "    axes[idx].set_title(f'Box Plot of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel(col)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "for idx in range(len(numerical_features), 9):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Univariate Analysis - Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "categorical_features = ['Gender', 'Category', 'Time_of_Day', 'Email_Interaction', \n",
    "                        'Device_Type', 'Payment_Method', 'Referral_Source']\n",
    "\n",
    "# Value counts for each categorical feature\n",
    "for col in categorical_features:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{col} - Value Counts:\")\n",
    "    print('='*60)\n",
    "    print(train_df[col].value_counts())\n",
    "    print(f\"\\nPercentage distribution:\")\n",
    "    print(train_df[col].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize categorical distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    value_counts = train_df[col].value_counts()\n",
    "    axes[idx].bar(range(len(value_counts)), value_counts.values, alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].set_xticks(range(len(value_counts)))\n",
    "    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for idx in range(len(categorical_features), 9):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bivariate Analysis - Features vs Purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchase rate by numerical features (binned)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    # Create bins\n",
    "    train_df[f'{col}_binned'] = pd.qcut(train_df[col], q=10, duplicates='drop')\n",
    "    \n",
    "    # Calculate purchase rate by bin\n",
    "    purchase_by_bin = train_df.groupby(f'{col}_binned')['Purchase'].mean() * 100\n",
    "    \n",
    "    # Plot\n",
    "    purchase_by_bin.plot(kind='bar', ax=axes[idx], alpha=0.7)\n",
    "    axes[idx].set_title(f'Purchase Rate by {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Purchase Rate (%)')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    axes[idx].axhline(y=purchase_rate*100, color='r', linestyle='--', label='Overall Avg')\n",
    "    axes[idx].legend()\n",
    "    \n",
    "    # Drop temporary column\n",
    "    train_df.drop(f'{col}_binned', axis=1, inplace=True)\n",
    "\n",
    "for idx in range(len(numerical_features), 9):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purchase rate by categorical features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    purchase_by_cat = train_df.groupby(col)['Purchase'].mean() * 100\n",
    "    purchase_by_cat.plot(kind='bar', ax=axes[idx], alpha=0.7)\n",
    "    axes[idx].set_title(f'Purchase Rate by {col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Purchase Rate (%)')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    axes[idx].axhline(y=purchase_rate*100, color='r', linestyle='--', label='Overall Avg')\n",
    "    axes[idx].legend()\n",
    "\n",
    "for idx in range(len(categorical_features), 9):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for categorical features\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "print(\"Chi-square test results for categorical features:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for col in categorical_features:\n",
    "    contingency_table = pd.crosstab(train_df[col], train_df['Purchase'])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.4e}\")\n",
    "    print(f\"  Significant: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "correlation_features = numerical_features + ['Purchase', 'Gender', 'Email_Interaction', 'Category']\n",
    "corr_matrix = train_df[correlation_features].corr()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target variable\n",
    "target_corr = corr_matrix['Purchase'].sort_values(ascending=False)\n",
    "print(\"Correlation with Purchase:\")\n",
    "print(\"=\"*40)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations with target\n",
    "plt.figure(figsize=(10, 8))\n",
    "target_corr.drop('Purchase').plot(kind='barh', alpha=0.7)\n",
    "plt.title('Feature Correlation with Purchase', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.ylabel('Features')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Price & Discount Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effective price (price after discount)\n",
    "train_df['Effective_Price'] = train_df['Price'] * (1 - train_df['Discount'] / 100)\n",
    "\n",
    "# Analyze price and discount effects\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Price distribution by purchase\n",
    "train_df[train_df['Purchase']==0]['Price'].hist(bins=50, alpha=0.5, label='No Purchase', ax=axes[0,0])\n",
    "train_df[train_df['Purchase']==1]['Price'].hist(bins=50, alpha=0.5, label='Purchase', ax=axes[0,0])\n",
    "axes[0,0].set_title('Price Distribution by Purchase')\n",
    "axes[0,0].set_xlabel('Price (€)')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Discount distribution by purchase\n",
    "train_df[train_df['Purchase']==0]['Discount'].hist(bins=50, alpha=0.5, label='No Purchase', ax=axes[0,1])\n",
    "train_df[train_df['Purchase']==1]['Discount'].hist(bins=50, alpha=0.5, label='Purchase', ax=axes[0,1])\n",
    "axes[0,1].set_title('Discount Distribution by Purchase')\n",
    "axes[0,1].set_xlabel('Discount (%)')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Effective price distribution by purchase\n",
    "train_df[train_df['Purchase']==0]['Effective_Price'].hist(bins=50, alpha=0.5, label='No Purchase', ax=axes[0,2])\n",
    "train_df[train_df['Purchase']==1]['Effective_Price'].hist(bins=50, alpha=0.5, label='Purchase', ax=axes[0,2])\n",
    "axes[0,2].set_title('Effective Price Distribution by Purchase')\n",
    "axes[0,2].set_xlabel('Effective Price (€)')\n",
    "axes[0,2].legend()\n",
    "\n",
    "# Scatter plots\n",
    "axes[1,0].scatter(train_df[train_df['Purchase']==0]['Price'], \n",
    "                  train_df[train_df['Purchase']==0]['Discount'], \n",
    "                  alpha=0.3, label='No Purchase', s=10)\n",
    "axes[1,0].scatter(train_df[train_df['Purchase']==1]['Price'], \n",
    "                  train_df[train_df['Purchase']==1]['Discount'], \n",
    "                  alpha=0.3, label='Purchase', s=10)\n",
    "axes[1,0].set_title('Price vs Discount')\n",
    "axes[1,0].set_xlabel('Price (€)')\n",
    "axes[1,0].set_ylabel('Discount (%)')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Purchase rate by discount brackets\n",
    "train_df['Discount_Bracket'] = pd.cut(train_df['Discount'], bins=[0, 10, 20, 30, 40, 100])\n",
    "discount_purchase = train_df.groupby('Discount_Bracket')['Purchase'].mean() * 100\n",
    "discount_purchase.plot(kind='bar', ax=axes[1,1], alpha=0.7)\n",
    "axes[1,1].set_title('Purchase Rate by Discount Bracket')\n",
    "axes[1,1].set_ylabel('Purchase Rate (%)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].axhline(y=purchase_rate*100, color='r', linestyle='--')\n",
    "\n",
    "# Purchase rate by price brackets\n",
    "train_df['Price_Bracket'] = pd.qcut(train_df['Price'], q=5, duplicates='drop')\n",
    "price_purchase = train_df.groupby('Price_Bracket')['Purchase'].mean() * 100\n",
    "price_purchase.plot(kind='bar', ax=axes[1,2], alpha=0.7)\n",
    "axes[1,2].set_title('Purchase Rate by Price Bracket')\n",
    "axes[1,2].set_ylabel('Purchase Rate (%)')\n",
    "axes[1,2].tick_params(axis='x', rotation=45)\n",
    "axes[1,2].axhline(y=purchase_rate*100, color='r', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up temporary columns\n",
    "train_df.drop(['Discount_Bracket', 'Price_Bracket'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering\n",
    "\n",
    "### 10.1 Price-Related Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_price_features(df):\n",
    "    \"\"\"\n",
    "    Create features related to price and discount\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Effective price after discount\n",
    "    df['Effective_Price'] = df['Price'] * (1 - df['Discount'] / 100)\n",
    "    \n",
    "    # Discount amount in euros\n",
    "    df['Discount_Amount'] = df['Price'] * df['Discount'] / 100\n",
    "    \n",
    "    # Price per item in cart\n",
    "    df['Price_Per_Item'] = df['Price'] / (df['Items_In_Cart'] + 1)  # +1 to avoid division by zero\n",
    "    \n",
    "    # Binary: High discount (>30%)\n",
    "    df['High_Discount'] = (df['Discount'] > 30).astype(int)\n",
    "    \n",
    "    # Binary: Has discount\n",
    "    df['Has_Discount'] = (df['Discount'] > 0).astype(int)\n",
    "    \n",
    "    # Price categories\n",
    "    df['Price_Category'] = pd.cut(df['Price'], \n",
    "                                   bins=[0, 50, 200, 500, 10000], \n",
    "                                   labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "train_with_price_features = create_price_features(train_df)\n",
    "print(\"New price-related features created:\")\n",
    "print(train_with_price_features[['Price', 'Discount', 'Effective_Price', 'Discount_Amount', \n",
    "                                  'Price_Per_Item', 'High_Discount', 'Has_Discount', 'Price_Category']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Engagement & Behavior Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_engagement_features(df):\n",
    "    \"\"\"\n",
    "    Create features related to user engagement and behavior\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Reviews engagement\n",
    "    df['Reviews_Engaged'] = (df['Reviews_Read'] > 0).astype(int)\n",
    "    df['Heavy_Reviewer'] = (df['Reviews_Read'] > df['Reviews_Read'].median()).astype(int)\n",
    "    \n",
    "    # Cart engagement\n",
    "    df['Has_Items_In_Cart'] = (df['Items_In_Cart'] > 0).astype(int)\n",
    "    df['Multiple_Items_In_Cart'] = (df['Items_In_Cart'] > 1).astype(int)\n",
    "    \n",
    "    # Engagement score categories\n",
    "    df['Engagement_Level'] = pd.qcut(df['Engagement_Score'], \n",
    "                                     q=4, \n",
    "                                     labels=['Low', 'Medium', 'High', 'Very_High'],\n",
    "                                     duplicates='drop')\n",
    "    \n",
    "    # Combined engagement metric\n",
    "    df['Total_Engagement'] = (df['Reviews_Read'] + df['Items_In_Cart'] + \n",
    "                              df['Email_Interaction'] * 5) * df['Engagement_Score']\n",
    "    \n",
    "    # Interaction with email during campaign\n",
    "    df['Email_During_Campaign'] = df['Email_Interaction'] & df['Campaign_Period']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "train_with_engagement = create_engagement_features(train_with_price_features)\n",
    "print(\"New engagement features created:\")\n",
    "print(train_with_engagement[['Reviews_Read', 'Items_In_Cart', 'Engagement_Score',\n",
    "                             'Reviews_Engaged', 'Has_Items_In_Cart', 'Total_Engagement']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Create features related to time and campaign periods\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Days since start\n",
    "    df['Days_Since_Start'] = df['Day']\n",
    "    \n",
    "    # Days into campaign (if in campaign)\n",
    "    df['Days_Into_Campaign'] = 0\n",
    "    df.loc[(df['Day'] >= 25) & (df['Day'] <= 50), 'Days_Into_Campaign'] = df['Day'] - 25\n",
    "    df.loc[(df['Day'] >= 75) & (df['Day'] <= 90), 'Days_Into_Campaign'] = df['Day'] - 75\n",
    "    \n",
    "    # Which campaign (0=none, 1=first, 2=second)\n",
    "    df['Campaign_Number'] = 0\n",
    "    df.loc[(df['Day'] >= 25) & (df['Day'] <= 50), 'Campaign_Number'] = 1\n",
    "    df.loc[(df['Day'] >= 75) & (df['Day'] <= 90), 'Campaign_Number'] = 2\n",
    "    \n",
    "    # Time of day indicators\n",
    "    df['Is_Morning'] = (df['Time_of_Day'] == 'morning').astype(int)\n",
    "    df['Is_Evening'] = (df['Time_of_Day'] == 'evening').astype(int)\n",
    "    df['Is_Afternoon'] = (df['Time_of_Day'] == 'afternoon').astype(int)\n",
    "    \n",
    "    # Weekend proxy (assuming 7-day weeks, days 6-7, 13-14, etc. are weekends)\n",
    "    df['Is_Weekend'] = ((df['Day'] % 7 == 6) | (df['Day'] % 7 == 0)).astype(int)\n",
    "    \n",
    "    # Week number\n",
    "    df['Week_Number'] = (df['Day'] - 1) // 7 + 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "train_with_temporal = create_temporal_features(train_with_engagement)\n",
    "print(\"New temporal features created:\")\n",
    "print(train_with_temporal[['Day', 'Campaign_Period', 'Campaign_Number', 'Days_Into_Campaign',\n",
    "                           'Time_of_Day', 'Is_Morning', 'Is_Weekend', 'Week_Number']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Customer Segment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_features(df):\n",
    "    \"\"\"\n",
    "    Create features related to customer demographics and segmentation\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Age groups\n",
    "    df['Age_Group'] = pd.cut(df['Age'], \n",
    "                             bins=[0, 25, 35, 45, 55, 100], \n",
    "                             labels=['18-25', '26-35', '36-45', '46-55', '56+'])\n",
    "    \n",
    "    # Young customer\n",
    "    df['Is_Young'] = (df['Age'] < 30).astype(int)\n",
    "    \n",
    "    # Senior customer\n",
    "    df['Is_Senior'] = (df['Age'] > 55).astype(int)\n",
    "    \n",
    "    # Socioeconomic categories\n",
    "    df['SES_Category'] = pd.qcut(df['Socioeconomic_Status_Score'], \n",
    "                                  q=3, \n",
    "                                  labels=['Low_SES', 'Mid_SES', 'High_SES'],\n",
    "                                  duplicates='drop')\n",
    "    \n",
    "    # High SES flag\n",
    "    df['Is_High_SES'] = (df['Socioeconomic_Status_Score'] > \n",
    "                         df['Socioeconomic_Status_Score'].quantile(0.75)).astype(int)\n",
    "    \n",
    "    # Customer value score (combining SES and engagement)\n",
    "    df['Customer_Value_Score'] = (df['Socioeconomic_Status_Score'] * 0.5 + \n",
    "                                  df['Engagement_Score'] * 0.5)\n",
    "    \n",
    "    # Gender-Age interaction\n",
    "    df['Female_Young'] = ((df['Gender'] == 1) & (df['Age'] < 35)).astype(int)\n",
    "    df['Male_Senior'] = ((df['Gender'] == 0) & (df['Age'] > 50)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "train_with_customer = create_customer_features(train_with_temporal)\n",
    "print(\"New customer segment features created:\")\n",
    "print(train_with_customer[['Age', 'Gender', 'Socioeconomic_Status_Score',\n",
    "                           'Age_Group', 'SES_Category', 'Customer_Value_Score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Device & Channel Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_channel_features(df):\n",
    "    \"\"\"\n",
    "    Create features related to device, payment, and referral channels\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Device type indicators\n",
    "    df['Is_Mobile'] = (df['Device_Type'] == 'Mobile').astype(int)\n",
    "    df['Is_Desktop'] = (df['Device_Type'] == 'Desktop').astype(int)\n",
    "    df['Is_Tablet'] = (df['Device_Type'] == 'Tablet').astype(int)\n",
    "    \n",
    "    # Payment method categories\n",
    "    df['Digital_Payment'] = df['Payment_Method'].isin(['Credit', 'PayPal']).astype(int)\n",
    "    df['Traditional_Payment'] = df['Payment_Method'].isin(['Bank', 'Cash']).astype(int)\n",
    "    \n",
    "    # Referral source categories\n",
    "    df['Organic_Traffic'] = df['Referral_Source'].isin(['Direct', 'Search_engine']).astype(int)\n",
    "    df['Paid_Traffic'] = df['Referral_Source'].isin(['Ads', 'Email']).astype(int)\n",
    "    df['Social_Traffic'] = (df['Referral_Source'] == 'Social_media').astype(int)\n",
    "    \n",
    "    # Email referral combined with email interaction\n",
    "    df['Email_Engaged_User'] = ((df['Referral_Source'] == 'Email') & \n",
    "                                (df['Email_Interaction'] == 1)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "train_with_channels = create_channel_features(train_with_customer)\n",
    "print(\"New channel features created:\")\n",
    "print(train_with_channels[['Device_Type', 'Payment_Method', 'Referral_Source',\n",
    "                           'Is_Mobile', 'Digital_Payment', 'Organic_Traffic']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Create interaction features between different variables\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Price and discount interaction\n",
    "    df['Price_Discount_Interaction'] = df['Price'] * df['Discount']\n",
    "    \n",
    "    # Engagement and SES interaction\n",
    "    df['Engagement_SES_Interaction'] = df['Engagement_Score'] * df['Socioeconomic_Status_Score']\n",
    "    \n",
    "    # Reviews and items interaction\n",
    "    df['Reviews_Items_Ratio'] = df['Reviews_Read'] / (df['Items_In_Cart'] + 1)\n",
    "    \n",
    "    # Age and price interaction\n",
    "    df['Age_Price_Interaction'] = df['Age'] * df['Price']\n",
    "    \n",
    "    # Campaign and discount interaction\n",
    "    df['Campaign_Discount_Interaction'] = df['Campaign_Period'].astype(int) * df['Discount']\n",
    "    \n",
    "    # Mobile and evening interaction (mobile evening shopping)\n",
    "    df['Mobile_Evening'] = ((df['Device_Type'] == 'Mobile') & \n",
    "                            (df['Time_of_Day'] == 'evening')).astype(int)\n",
    "    \n",
    "    # High engagement with items in cart\n",
    "    df['High_Engagement_With_Cart'] = ((df['Engagement_Score'] > df['Engagement_Score'].median()) & \n",
    "                                       (df['Items_In_Cart'] > 0)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test the function\n",
    "train_with_interactions = create_interaction_features(train_with_channels)\n",
    "print(\"New interaction features created:\")\n",
    "print(train_with_interactions[['Price', 'Discount', 'Engagement_Score',\n",
    "                               'Price_Discount_Interaction', 'Engagement_SES_Interaction']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 Combined Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Apply all feature engineering steps\n",
    "    \"\"\"\n",
    "    df = create_price_features(df)\n",
    "    df = create_engagement_features(df)\n",
    "    df = create_temporal_features(df)\n",
    "    df = create_customer_features(df)\n",
    "    df = create_channel_features(df)\n",
    "    df = create_interaction_features(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to train and test sets\n",
    "train_engineered = engineer_features(train_df)\n",
    "test_engineered = engineer_features(test_df)\n",
    "\n",
    "print(f\"Original training set shape: {train_df.shape}\")\n",
    "print(f\"Engineered training set shape: {train_engineered.shape}\")\n",
    "print(f\"\\nNumber of new features created: {train_engineered.shape[1] - train_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display new features\n",
    "new_features = [col for col in train_engineered.columns if col not in train_df.columns]\n",
    "print(f\"\\nNew features ({len(new_features)} total):\")\n",
    "print(\"=\"*60)\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick random forest to assess feature importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare data for quick model\n",
    "rf_df = train_engineered.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = ['Time_of_Day', 'Device_Type', 'Payment_Method', 'Referral_Source',\n",
    "                   'Price_Category', 'Engagement_Level', 'Age_Group', 'SES_Category']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in rf_df.columns:\n",
    "        rf_df[col] = le.fit_transform(rf_df[col].astype(str))\n",
    "\n",
    "# Drop non-feature columns\n",
    "feature_cols = [col for col in rf_df.columns if col not in ['Purchase', 'Session_ID']]\n",
    "X = rf_df[feature_cols]\n",
    "y = rf_df['Purchase']\n",
    "\n",
    "# Train quick random forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 30 Most Important Features:\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = feature_importance.head(25)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'], alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Top 25 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile key insights\n",
    "print(\"=\"*80)\n",
    "print(\"KEY INSIGHTS FROM EDA & FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. TARGET VARIABLE\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   - Overall purchase rate: {purchase_rate:.2%}\")\n",
    "print(f\"   - Class imbalance ratio: {(1-purchase_rate)/purchase_rate:.1f}:1\")\n",
    "\n",
    "print(\"\\n2. CAMPAIGN EFFECTIVENESS\")\n",
    "print(\"-\" * 80)\n",
    "campaign_stats = train_df.groupby('Campaign_Period')['Purchase'].mean()\n",
    "print(f\"   - Purchase rate during campaigns: {campaign_stats[True]:.2%}\")\n",
    "print(f\"   - Purchase rate outside campaigns: {campaign_stats[False]:.2%}\")\n",
    "print(f\"   - Campaign lift: {(campaign_stats[True]/campaign_stats[False]-1)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n3. TOP CORRELATED FEATURES WITH PURCHASE\")\n",
    "print(\"-\" * 80)\n",
    "print(target_corr.head(6))\n",
    "\n",
    "print(\"\\n4. FEATURES CREATED\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   - Original features: {train_df.shape[1]}\")\n",
    "print(f\"   - Engineered features: {train_engineered.shape[1]}\")\n",
    "print(f\"   - New features added: {train_engineered.shape[1] - train_df.shape[1]}\")\n",
    "\n",
    "print(\"\\n5. TOP FEATURE CATEGORIES (by importance)\")\n",
    "print(\"-\" * 80)\n",
    "for i, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"   {i+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save engineered datasets\n",
    "train_engineered.to_csv('../data/train_engineered.csv', index=False)\n",
    "test_engineered.to_csv('../data/test_engineered.csv', index=False)\n",
    "\n",
    "print(\"Engineered datasets saved!\")\n",
    "print(f\"Train: {train_engineered.shape}\")\n",
    "print(f\"Test: {test_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "### For Tomorrow's Team Discussion:\n",
    "\n",
    "1. **Feature Selection**\n",
    "   - Compare which features each team member found most useful\n",
    "   - Decide on a unified feature set or test multiple combinations\n",
    "   - Consider dimensionality reduction (PCA, feature selection methods)\n",
    "\n",
    "2. **Model Selection**\n",
    "   - Test multiple algorithms (Logistic Regression, Random Forest, XGBoost, etc.)\n",
    "   - Consider ensemble methods\n",
    "   - Set up proper cross-validation\n",
    "\n",
    "3. **Class Imbalance Handling**\n",
    "   - Test SMOTE, class weights, or undersampling\n",
    "   - Evaluate using appropriate metrics (F1, ROC-AUC, Precision-Recall)\n",
    "\n",
    "4. **Business Constraints**\n",
    "   - Set threshold to achieve ≤€200/day budget (max 2000 sessions)\n",
    "   - Optimize for precision to reduce wasted ad spend\n",
    "   - Create cost-benefit analysis\n",
    "\n",
    "5. **Insights & Playbook**\n",
    "   - Develop 3-5 actionable marketing rules from feature importance\n",
    "   - Create customer segments for targeted marketing\n",
    "   - Document findings for the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Ideas\n",
    "\n",
    "Use this section to document your own observations and ideas:\n",
    "\n",
    "- \n",
    "- \n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
